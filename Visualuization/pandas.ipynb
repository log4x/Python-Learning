{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "356a4f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ebec7",
   "metadata": {},
   "source": [
    "## Inroduction | Creating Objects | Viewing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f47e14",
   "metadata": {},
   "source": [
    "1. https://www.geeksforgeeks.org/pandas/introduction-to-pandas-in-python/\n",
    "\n",
    "2. https://www.geeksforgeeks.org/python/how-to-install-python-pandas-on-windows-and-linux/\n",
    "\n",
    "3. https://www.geeksforgeeks.org/machine-learning/how-to-use-jupyter-notebook-an-ultimate-guide/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0993340",
   "metadata": {},
   "source": [
    "1. https://www.geeksforgeeks.org/pandas/creating-a-pandas-dataframe/\n",
    "\n",
    "2. https://www.geeksforgeeks.org/python/python-pandas-series/\n",
    "\n",
    "3. https://www.geeksforgeeks.org/python/creating-a-pandas-series/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b9e10",
   "metadata": {},
   "source": [
    "1. https://www.geeksforgeeks.org/python/python-pandas-dataframe-series-head-method/\n",
    "\n",
    "2. https://www.geeksforgeeks.org/python/python-pandas-dataframe-series-tail-method/\n",
    "\n",
    "3. https://www.geeksforgeeks.org/pandas/python-pandas-dataframe-describe-method/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c2ce791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PANDAS BASICS: SERIES & DATAFRAME CREATION ===\n",
      "\n",
      "Series from list:\n",
      " 0    10\n",
      "1    20\n",
      "2    30\n",
      "3    40\n",
      "dtype: int64 \n",
      "\n",
      "Series with custom index & dtype:\n",
      " a    100\n",
      "b    200\n",
      "c    300\n",
      "Name: scores, dtype: int64 \n",
      "\n",
      "Series from dict (keys become index):\n",
      " apple     5\n",
      "banana    3\n",
      "cherry    7\n",
      "Name: fruits, dtype: int64 \n",
      "\n",
      "s2['b'] -> 200\n",
      "s1[0]    -> 10\n",
      "s2.index -> Index(['a', 'b', 'c'], dtype='object')\n",
      "s2.values -> [100 200 300]\n",
      "\n",
      "\n",
      "DataFrame from dict of lists:\n",
      "       Name  Age       City\n",
      "0    Alice   25      Delhi\n",
      "1      Bob   30     Mumbai\n",
      "2  Charlie   22  Bangalore \n",
      "\n",
      "DataFrame from list of dicts (records):\n",
      "   Name  Age     City\n",
      "0  Dan   28      NaN\n",
      "1  Eve   35  Chennai \n",
      "\n",
      "DataFrame from 2D ndarray:\n",
      "    A  B  C\n",
      "0  1  2  3\n",
      "1  4  5  6 \n",
      "\n",
      "DataFrame from Series objects (index aligned):\n",
      "    col1  col2\n",
      "x    10   0.1\n",
      "y    20   0.2\n",
      "z    30   0.3 \n",
      "\n",
      "df1.shape: (3, 3)\n",
      "df1.columns: Index(['Name', 'Age', 'City'], dtype='object')\n",
      "df1.dtypes:\n",
      " Name    object\n",
      "Age      int64\n",
      "City    object\n",
      "dtype: object\n",
      "\n",
      "Concise info() output:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Name    3 non-null      object\n",
      " 1   Age     3 non-null      int64 \n",
      " 2   City    3 non-null      object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 204.0+ bytes\n",
      "\n",
      "-- head() examples --\n",
      "df1.head()  -> default first 5 rows (here all rows):\n",
      "       Name  Age       City\n",
      "0    Alice   25      Delhi\n",
      "1      Bob   30     Mumbai\n",
      "2  Charlie   22  Bangalore \n",
      "\n",
      "df1.head(2) -> first 2 rows:\n",
      "     Name  Age    City\n",
      "0  Alice   25   Delhi\n",
      "1    Bob   30  Mumbai \n",
      "\n",
      "-- tail() examples --\n",
      "df1.tail()  -> last 5 rows (here all):\n",
      "       Name  Age       City\n",
      "0    Alice   25      Delhi\n",
      "1      Bob   30     Mumbai\n",
      "2  Charlie   22  Bangalore \n",
      "\n",
      "df1.tail(1) -> last 1 row:\n",
      "       Name  Age       City\n",
      "2  Charlie   22  Bangalore \n",
      "\n",
      "-- describe() example --\n",
      "             Age\n",
      "count   3.000000\n",
      "mean   25.666667\n",
      "std     4.041452\n",
      "min    22.000000\n",
      "25%    23.500000\n",
      "50%    25.000000\n",
      "75%    27.500000\n",
      "max    30.000000 \n",
      "\n",
      "Select single column (Series):\n",
      " 0    25\n",
      "1    30\n",
      "2    22\n",
      "Name: Age, dtype: int64 \n",
      "\n",
      "Select multiple columns (DataFrame):\n",
      "       Name       City\n",
      "0    Alice      Delhi\n",
      "1      Bob     Mumbai\n",
      "2  Charlie  Bangalore \n",
      "\n",
      "Row 0 by position (iloc):\n",
      " Name    Alice\n",
      "Age        25\n",
      "City    Delhi\n",
      "Name: 0, dtype: object \n",
      "\n",
      "Select rows where Age > 24:\n",
      "     Name  Age    City\n",
      "0  Alice   25   Delhi\n",
      "1    Bob   30  Mumbai \n",
      "\n",
      "After adding Age_plus_5 column:\n",
      "       Name  Age       City  Age_plus_5\n",
      "0    Alice   25      Delhi          30\n",
      "1      Bob   30     Mumbai          35\n",
      "2  Charlie   22  Bangalore          27 \n",
      "\n",
      "After drop (copy):\n",
      "       Name  Age       City\n",
      "0    Alice   25      Delhi\n",
      "1      Bob   30     Mumbai\n",
      "2  Charlie   22  Bangalore \n",
      "\n",
      "=== COOKBOOK ===\n",
      "Value counts of a column (frequency): df['City'].value_counts() ->\n",
      "City\n",
      "Delhi        1\n",
      "Mumbai       1\n",
      "Bangalore    1\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Sort by column: df.sort_values('Age') ->\n",
      "      Name  Age       City  Age_plus_5\n",
      "2  Charlie   22  Bangalore          27\n",
      "0    Alice   25      Delhi          30\n",
      "1      Bob   30     Mumbai          35 \n",
      "\n",
      "Reset index: df.reset_index(drop=True) ->\n",
      "      Name  Age       City  Age_plus_5\n",
      "0    Alice   25      Delhi          30\n",
      "1      Bob   30     Mumbai          35\n",
      "2  Charlie   22  Bangalore          27 \n",
      "\n",
      "Rename columns: df.rename(columns={'Age':'age_years'}) ->\n",
      "      Name  age_years       City  Age_plus_5\n",
      "0    Alice         25      Delhi          30\n",
      "1      Bob         30     Mumbai          35\n",
      "2  Charlie         22  Bangalore          27 \n",
      "\n",
      "\n",
      "=== Done: pandas basics example script ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas_basics_combined.py\n",
    "# Pandas basics: installation notes, Series, DataFrame creation, head/tail/describe examples.\n",
    "# Top-level script with detailed inline comments explaining what each function does and parameters.\n",
    "\n",
    "# ---------- INSTALL / START NOTES (no code) ----------\n",
    "# To install pandas:\n",
    "#   pip install pandas\n",
    "#\n",
    "# Recommended environment:\n",
    "#   - Use a virtualenv or conda environment (conda create -n pd python=3.10)\n",
    "#   - Use Jupyter Notebook or JupyterLab for interactive exploration:\n",
    "#       jupyter notebook\n",
    "#   - See GfG guides for step-by-step install and using Jupyter. :contentReference[oaicite:1]{index=1}\n",
    "#\n",
    "# Note: pandas depends on numpy. If you have Anaconda, pandas is included.\n",
    "\n",
    "# ------------------------------\n",
    "# Imports\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n=== PANDAS BASICS: SERIES & DATAFRAME CREATION ===\\n\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1) PANDAS SERIES\n",
    "# ======================================================\n",
    "# A Series is a 1D labeled array. It holds values + an index (labels).\n",
    "# Common constructor:\n",
    "#   pd.Series(data, index=None, dtype=None, name=None)\n",
    "# Parameters:\n",
    "#   data  : list/ndarray/dict/scalar\n",
    "#   index : list-like labels; if omitted pandas uses 0..n-1\n",
    "#   dtype : force data type (e.g., 'float64', 'int32', 'object')\n",
    "#   name  : optional name of the Series (shows in prints)\n",
    "#\n",
    "# Series behaves like a single column (vector). See GfG Series guide. :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "# Create from a Python list (default index = 0..n-1)\n",
    "s1 = pd.Series([10, 20, 30, 40])\n",
    "print(\"Series from list:\\n\", s1, \"\\n\")\n",
    "\n",
    "# Create from a list with a custom index\n",
    "s2 = pd.Series([100, 200, 300], index=['a', 'b', 'c'], dtype='int64', name='scores')\n",
    "print(\"Series with custom index & dtype:\\n\", s2, \"\\n\")\n",
    "\n",
    "# Create from a dict (keys -> index, values -> data)\n",
    "d = {'apple': 5, 'banana': 3, 'cherry': 7}\n",
    "s3 = pd.Series(d, name='fruits')\n",
    "print(\"Series from dict (keys become index):\\n\", s3, \"\\n\")\n",
    "\n",
    "\n",
    "# Accessing Series:\n",
    "print(\"s2['b'] ->\", s2['b'])           # by label\n",
    "print(\"s1[0]    ->\", s1[0])            # by integer position (index 0)\n",
    "print(\"s2.index ->\", s2.index)         # index labels\n",
    "print(\"s2.values ->\", s2.values)       # ndarray of values\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2) PANDAS DATAFRAME (many ways to create)\n",
    "# ======================================================\n",
    "# A DataFrame is a 2D labeled tabular structure (rows + columns).\n",
    "# Constructor signatures:\n",
    "#   pd.DataFrame(data=None, index=None, columns=None, dtype=None)\n",
    "# where 'data' can be:\n",
    "#   - dict of lists/ndarrays: {colname: column_values}\n",
    "#   - list of dicts (records)\n",
    "#   - 2D ndarray + columns list\n",
    "#   - Series dict\n",
    "#\n",
    "# GfG covers many ways to create DataFrame. :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "# 2A: From dict of lists (common)\n",
    "data_dict = {\n",
    "    'Name' : ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age'  : [25, 30, 22],\n",
    "    'City' : ['Delhi', 'Mumbai', 'Bangalore']\n",
    "}\n",
    "df1 = pd.DataFrame(data_dict, columns=['Name', 'Age', 'City'])  # columns order optional\n",
    "print(\"DataFrame from dict of lists:\\n\", df1, \"\\n\")\n",
    "\n",
    "# 2B: From list of dicts (each dict is a row / \"record\")\n",
    "rows = [\n",
    "    {'Name':'Dan', 'Age': 28},\n",
    "    {'Name':'Eve', 'Age': 35, 'City': 'Chennai'},  # missing City in first row -> NaN\n",
    "]\n",
    "df2 = pd.DataFrame(rows)\n",
    "print(\"DataFrame from list of dicts (records):\\n\", df2, \"\\n\")\n",
    "\n",
    "# 2C: From NumPy 2D array + column names\n",
    "arr = np.array([[1,2,3],[4,5,6]])\n",
    "df3 = pd.DataFrame(arr, columns=['A','B','C'])\n",
    "print(\"DataFrame from 2D ndarray:\\n\", df3, \"\\n\")\n",
    "\n",
    "# 2D: From a Series mapping (each Series is a column)\n",
    "col1 = pd.Series([10,20,30], index=['x','y','z'])\n",
    "col2 = pd.Series([0.1, 0.2, 0.3], index=['x','y','z'])\n",
    "df4 = pd.DataFrame({'col1': col1, 'col2': col2})\n",
    "print(\"DataFrame from Series objects (index aligned):\\n\", df4, \"\\n\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 3) Basic DataFrame inspection / attributes\n",
    "# ======================================================\n",
    "# Useful attributes & methods:\n",
    "#   df.shape      -> (n_rows, n_cols)\n",
    "#   df.columns    -> column Index\n",
    "#   df.index      -> row Index\n",
    "#   df.dtypes     -> dtype per column\n",
    "#   df.info()     -> concise summary (non-null counts + dtypes)\n",
    "#   df.head(n=5)  -> first n rows (default n=5)\n",
    "#   df.tail(n=5)  -> last n rows (default n=5)\n",
    "#   df.describe() -> summary statistics for numeric columns (count, mean, std, min, 25%, 50%, 75%, max)\n",
    "#\n",
    "# head/tail/describe are covered on GfG. :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "print(\"df1.shape:\", df1.shape)\n",
    "print(\"df1.columns:\", df1.columns)\n",
    "print(\"df1.dtypes:\\n\", df1.dtypes)\n",
    "print(\"\\nConcise info() output:\")\n",
    "df1.info()   # prints info (non-null counts, memory usage)\n",
    "\n",
    "print(\"\\n-- head() examples --\")\n",
    "print(\"df1.head()  -> default first 5 rows (here all rows):\\n\", df1.head(), \"\\n\")\n",
    "print(\"df1.head(2) -> first 2 rows:\\n\", df1.head(2), \"\\n\")\n",
    "\n",
    "print(\"-- tail() examples --\")\n",
    "print(\"df1.tail()  -> last 5 rows (here all):\\n\", df1.tail(), \"\\n\")\n",
    "print(\"df1.tail(1) -> last 1 row:\\n\", df1.tail(1), \"\\n\")\n",
    "\n",
    "print(\"-- describe() example --\")\n",
    "# describe() returns descriptive stats for numeric columns by default\n",
    "print(df1.describe(), \"\\n\")   # count, mean, std, min, quartiles, max\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4) Small manipulation examples (selection / slicing)\n",
    "# ======================================================\n",
    "# Selecting columns: df['col'] returns a Series; df[['col1','col2']] returns DataFrame\n",
    "print(\"Select single column (Series):\\n\", df1['Age'], \"\\n\")\n",
    "print(\"Select multiple columns (DataFrame):\\n\", df1[['Name','City']], \"\\n\")\n",
    "\n",
    "# Row selection by position: iloc, by label: loc\n",
    "print(\"Row 0 by position (iloc):\\n\", df1.iloc[0], \"\\n\")\n",
    "# If index is labeled (not 0..n-1) use df.loc[label]\n",
    "print(\"Select rows where Age > 24:\\n\", df1[df1['Age'] > 24], \"\\n\")\n",
    "\n",
    "# Adding a new column (vectorized)\n",
    "df1['Age_plus_5'] = df1['Age'] + 5\n",
    "print(\"After adding Age_plus_5 column:\\n\", df1, \"\\n\")\n",
    "\n",
    "# Dropping a column (returns new DataFrame unless inplace=True)\n",
    "df_copy = df1.drop(columns=['Age_plus_5'])\n",
    "print(\"After drop (copy):\\n\", df_copy, \"\\n\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 5) IO quick notes (read/write)\n",
    "# ======================================================\n",
    "# Read CSV:\n",
    "#   pd.read_csv(filepath, sep=',', header='infer', index_col=None, usecols=None, dtype=None, parse_dates=False)\n",
    "# Important params:\n",
    "#   filepath   : path to CSV\n",
    "#   sep        : delimiter (default ',')\n",
    "#   header     : row number to use as column names (default 0)\n",
    "#   index_col  : column to use as row labels\n",
    "#   parse_dates: try to parse date columns\n",
    "#\n",
    "# Write CSV:\n",
    "#   df.to_csv(path, index=True/False)\n",
    "#\n",
    "# Example (commented out since no file in this run):\n",
    "#   df = pd.read_csv('data.csv')\n",
    "#   df.to_csv('out.csv', index=False)\n",
    "#\n",
    "# See GfG install/read guides for more. :contentReference[oaicite:5]{index=5}\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 6) Short cookbook: useful one-liners\n",
    "# ======================================================\n",
    "print(\"=== COOKBOOK ===\")\n",
    "print(\"Value counts of a column (frequency): df['City'].value_counts() ->\")\n",
    "print(df1['City'].value_counts(), \"\\n\")\n",
    "\n",
    "print(\"Sort by column: df.sort_values('Age') ->\")\n",
    "print(df1.sort_values('Age'), \"\\n\")\n",
    "\n",
    "print(\"Reset index: df.reset_index(drop=True) ->\")\n",
    "print(df1.reset_index(drop=True), \"\\n\")\n",
    "\n",
    "print(\"Rename columns: df.rename(columns={'Age':'age_years'}) ->\")\n",
    "print(df1.rename(columns={'Age':'age_years'}), \"\\n\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 7) Closing notes & pointers\n",
    "# ======================================================\n",
    "# - Pandas Series and DataFrame are built on top of NumPy arrays — operations are vectorized.\n",
    "# - Use head()/tail()/describe() for quick data exploration. They are your first commands after loading data. :contentReference[oaicite:6]{index=6}\n",
    "# - For learning path: start with Series -> DataFrame -> IO -> indexing -> groupby -> merge/join -> time-series.\n",
    "# - I can convert this into a Jupyter notebook with explanatory cells and outputs if you like.\n",
    "\n",
    "print(\"\\n=== Done: pandas basics example script ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814c68f",
   "metadata": {},
   "source": [
    "### Common binary Operations\n",
    "\n",
    "sub()\tMethod is used to subtract series or list like objects with same length from the caller series\n",
    "\n",
    "mul()\tMethod is used to multiply series or list like objects with same length with the caller series\n",
    "\n",
    "div()\tMethod is used to divide series or list like objects with same length by the caller series\n",
    "\n",
    "sum()\tReturns the sum of the values for the requested axis\n",
    "\n",
    "prod()\tReturns the product of the values for the requested axis\n",
    "\n",
    "mean()\tReturns the mean of the values for the requested axis\n",
    "\n",
    "pow()\tMethod is used to put each element of passed series as exponential power of caller series and returned the results\n",
    "\n",
    "abs()\tMethod is used to get the absolute numeric value of each element in Series/DataFrame\n",
    "\n",
    "cov()\tMethod is used to find covariance of two series\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "1. https://www.geeksforgeeks.org/python/python-pandas-series-mul/\n",
    "\n",
    "2. https://www.geeksforgeeks.org/python/python-pandas-series-div/\n",
    "\n",
    "3. https://www.geeksforgeeks.org/python/python-pandas-series-sum/\n",
    "\n",
    "4. https://www.geeksforgeeks.org/machine-learning/python-pandas-series-prod/\n",
    "\n",
    "5. https://www.geeksforgeeks.org/pandas/python-pandas-series-mean/\n",
    "\n",
    "6. https://www.geeksforgeeks.org/python/python-pandas-series-pow/\n",
    "\n",
    "7. https://www.geeksforgeeks.org/python/python-pandas-series-abs/\n",
    "\n",
    "8. https://www.geeksforgeeks.org/python/python-pandas-series-cov-to-find-covariance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b93554a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== PANDAS SERIES OPERATIONS ====================\n",
      "\n",
      "\n",
      "1) Series.mul() - elementwise multiplication\n",
      "s1:\n",
      " 0    10\n",
      "1    20\n",
      "2    30\n",
      "3    40\n",
      "dtype: int64\n",
      "\n",
      "Multiplying s1 * 2 → s1.mul(2)\n",
      "0    20\n",
      "1    40\n",
      "2    60\n",
      "3    80\n",
      "dtype: int64\n",
      "\n",
      "Multiplying two series s1 * s2\n",
      "0     10\n",
      "1     40\n",
      "2     90\n",
      "3    160\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "2) Series.div() - elementwise division\n",
      "s1 / 10 → s1.div(10)\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "3    4.0\n",
      "dtype: float64\n",
      "\n",
      "Dividing s1 / s2 → s1.div(s2)\n",
      "0    10.0\n",
      "1    10.0\n",
      "2    10.0\n",
      "3    10.0\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "3) Series.sum() - sum of elements\n",
      "Sum of s1: 100\n",
      "\n",
      "\n",
      "4) Series.prod() - product of all elements\n",
      "Product of s1: 240000\n",
      "\n",
      "\n",
      "5) Series.mean() - average value\n",
      "Mean of s1: 25.0\n",
      "\n",
      "\n",
      "6) Series.pow() - exponentiation\n",
      "s2.pow(2)  # each element squared\n",
      "0     1\n",
      "1     4\n",
      "2     9\n",
      "3    16\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "7) Series.abs() - absolute values\n",
      "Original:\n",
      " 0    -5\n",
      "1   -10\n",
      "2    15\n",
      "3    -2\n",
      "dtype: int64\n",
      "Absolute:\n",
      " 0     5\n",
      "1    10\n",
      "2    15\n",
      "3     2\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "8) Series.cov() - covariance between Series\n",
      "x:\n",
      " 0    10\n",
      "1    20\n",
      "2    30\n",
      "3    40\n",
      "4    50\n",
      "dtype: int64\n",
      "y:\n",
      " 0     5\n",
      "1    15\n",
      "2    25\n",
      "3    35\n",
      "4    45\n",
      "dtype: int64\n",
      "Covariance x.cov(y): 250.0\n",
      "\n",
      "==================== DONE ====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas_series_operations.py\n",
    "# Covers:\n",
    "# 1) Series.mul()\n",
    "# 2) Series.div()\n",
    "# 3) Series.sum()\n",
    "# 4) Series.prod()\n",
    "# 5) Series.mean()\n",
    "# 6) Series.pow()\n",
    "# 7) Series.abs()\n",
    "# 8) Series.cov()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"\\n==================== PANDAS SERIES OPERATIONS ====================\\n\")\n",
    "\n",
    "# Sample Series for demonstrations\n",
    "s1 = pd.Series([10, 20, 30, 40])\n",
    "s2 = pd.Series([1, 2, 3, 4])\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 1) Series.mul() → elementwise multiplication\n",
    "# =====================================================================\n",
    "# Syntax:\n",
    "#   Series.mul(other, fill_value=None)\n",
    "# Parameters:\n",
    "#   other       : number or another Series\n",
    "#   fill_value  : value used to fill missing indexes before operation\n",
    "# Meaning:\n",
    "#   Performs s1 * other elementwise.\n",
    "print(\"\\n1) Series.mul() - elementwise multiplication\")\n",
    "print(\"s1:\\n\", s1)\n",
    "print(\"\\nMultiplying s1 * 2 → s1.mul(2)\")\n",
    "print(s1.mul(2))\n",
    "\n",
    "print(\"\\nMultiplying two series s1 * s2\")\n",
    "print(s1.mul(s2))\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 2) Series.div() → elementwise division\n",
    "# =====================================================================\n",
    "# Syntax:\n",
    "#   Series.div(other, fill_value=None)\n",
    "# Meaning:\n",
    "#   s1 / other elementwise.\n",
    "print(\"\\n\\n2) Series.div() - elementwise division\")\n",
    "print(\"s1 / 10 → s1.div(10)\")\n",
    "print(s1.div(10))\n",
    "\n",
    "print(\"\\nDividing s1 / s2 → s1.div(s2)\")\n",
    "print(s1.div(s2))\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3) Series.sum() → sum of all elements\n",
    "# =====================================================================\n",
    "# Syntax:\n",
    "#   Series.sum(skipna=True)\n",
    "# Parameters:\n",
    "#   skipna : ignore NaN values (default True)\n",
    "# Meaning:\n",
    "#   Returns scalar sum.\n",
    "print(\"\\n\\n3) Series.sum() - sum of elements\")\n",
    "print(\"Sum of s1:\", s1.sum())\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 4) Series.prod() → product of all elements\n",
    "# =====================================================================\n",
    "# Syntax:\n",
    "#   Series.prod(skipna=True)\n",
    "# Meaning:\n",
    "#   Multiply all elements together → returns scalar.\n",
    "print(\"\\n\\n4) Series.prod() - product of all elements\")\n",
    "print(\"Product of s1:\", s1.prod())\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 5) Series.mean() → mean (average)\n",
    "# =====================================================================\n",
    "# Syntax:\n",
    "#   Series.mean(skipna=True)\n",
    "# Meaning:\n",
    "#   Returns arithmetic mean of the series.\n",
    "print(\"\\n\\n5) Series.mean() - average value\")\n",
    "print(\"Mean of s1:\", s1.mean())\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 6) Series.pow() → elementwise exponentiation\n",
    "# =====================================================================\n",
    "# Syntax:\n",
    "#   Series.pow(other, fill_value=None)\n",
    "# Meaning:\n",
    "#   s1 ** other (each element raised to power).\n",
    "print(\"\\n\\n6) Series.pow() - exponentiation\")\n",
    "print(\"s2.pow(2)  # each element squared\")\n",
    "print(s2.pow(2))\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 7) Series.abs() → absolute values\n",
    "# =====================================================================\n",
    "# Syntax:\n",
    "#   Series.abs()\n",
    "# Meaning:\n",
    "#   Returns absolute value of each element.\n",
    "s3 = pd.Series([-5, -10, 15, -2])\n",
    "print(\"\\n\\n7) Series.abs() - absolute values\")\n",
    "print(\"Original:\\n\", s3)\n",
    "print(\"Absolute:\\n\", s3.abs())\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 8) Series.cov() → covariance between two Series\n",
    "# =====================================================================\n",
    "# Syntax:\n",
    "#   Series.cov(other)\n",
    "# Meaning:\n",
    "#   Calculates covariance between this Series and another.\n",
    "#\n",
    "# Important:\n",
    "#   - Lengths must match\n",
    "#   - Returns scalar covariance value\n",
    "#\n",
    "# Covariance meaning:\n",
    "#   +ve → variables increase together\n",
    "#   -ve → one increases while other decreases\n",
    "#    0  → independent movement\n",
    "#\n",
    "x = pd.Series([10, 20, 30, 40, 50])\n",
    "y = pd.Series([5, 15, 25, 35, 45])\n",
    "\n",
    "print(\"\\n\\n8) Series.cov() - covariance between Series\")\n",
    "print(\"x:\\n\", x)\n",
    "print(\"y:\\n\", y)\n",
    "\n",
    "print(\"Covariance x.cov(y):\", x.cov(y))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n==================== DONE ====================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e0a4c",
   "metadata": {},
   "source": [
    "## Selection | Slicing | Other Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15ef6c",
   "metadata": {},
   "source": [
    "1. https://www.geeksforgeeks.org/pandas/dealing-with-rows-and-columns-in-pandas-dataframe/\n",
    "\n",
    "2. https://www.geeksforgeeks.org/python/python-pandas-extracting-rows-using-loc/\n",
    "\n",
    "3. https://www.geeksforgeeks.org/python/python-extracting-rows-using-pandas-iloc/\n",
    "\n",
    "4. https://www.geeksforgeeks.org/pandas/indexing-and-selecting-data-with-pandas/\n",
    "\n",
    "5. https://www.geeksforgeeks.org/pandas/boolean-indexing-in-pandas/\n",
    "\n",
    "6. https://www.geeksforgeeks.org/python/python-pandas-dataframe-ix/\n",
    "\n",
    "7. https://www.geeksforgeeks.org/python/python-pandas-series-str-slice/\n",
    "\n",
    "8. https://www.geeksforgeeks.org/python/how-to-take-column-slices-of-dataframe-in-pandas/\n",
    "\n",
    "\n",
    "### Other Operations\n",
    "\n",
    "1. https://www.geeksforgeeks.org/python/python-pandas-apply/\n",
    "\n",
    "2. https://www.geeksforgeeks.org/python/apply-function-to-every-row-in-a-pandas-dataframe/\n",
    "\n",
    "3. https://www.geeksforgeeks.org/pandas/python-pandas-series-apply/\n",
    "\n",
    "4. https://www.geeksforgeeks.org/python/python-pandas-dataframe-aggregate/\n",
    "\n",
    "5. https://www.geeksforgeeks.org/python/python-pandas-dataframe-mean/\n",
    "\n",
    "6. https://www.geeksforgeeks.org/pandas/python-pandas-series-mean/\n",
    "\n",
    "7. https://www.geeksforgeeks.org/python/python-pandas-dataframe-mad/\n",
    "\n",
    "8. https://www.geeksforgeeks.org/python/python-pandas-series-mad-to-calculate-mean-absolute-deviation-of-a-series/\n",
    "\n",
    "9. https://www.geeksforgeeks.org/python/python-pandas-dataframe-sem/\n",
    "\n",
    "18. https://www.geeksforgeeks.org/python/python-pandas-series-value_counts/\n",
    "\n",
    "10. https://www.geeksforgeeks.org/python/applying-lambda-functions-to-pandas-dataframe/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e136aa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SETUP: sample DataFrame ===\n",
      "\n",
      "Initial DataFrame:\n",
      "       Name  Age       City  Score\n",
      "0    Alice   25      Delhi     85\n",
      "1      Bob   30     Mumbai     92\n",
      "2  Charlie   35  Bengaluru     78\n",
      "3    David   40    Kolkata     88\n",
      "4      Eve   22    Chennai     91 \n",
      "\n",
      "shape: (5, 4)\n",
      "columns: Index(['Name', 'Age', 'City', 'Score'], dtype='object')\n",
      "index: RangeIndex(start=0, stop=5, step=1) \n",
      "\n",
      "\n",
      "=== 1) Rows & Columns operations ===\n",
      "\n",
      "Added 'Passed' column:\n",
      "       Name  Age       City  Score  Passed\n",
      "0    Alice   25      Delhi     85    True\n",
      "1      Bob   30     Mumbai     92    True\n",
      "2  Charlie   35  Bengaluru     78   False\n",
      "3    David   40    Kolkata     88    True\n",
      "4      Eve   22    Chennai     91    True \n",
      "\n",
      "After df.drop(columns=['Passed']) (copy):\n",
      "       Name  Age       City  Score\n",
      "0    Alice   25      Delhi     85\n",
      "1      Bob   30     Mumbai     92\n",
      "2  Charlie   35  Bengaluru     78\n",
      "3    David   40    Kolkata     88\n",
      "4      Eve   22    Chennai     91 \n",
      "\n",
      "After df.drop(index=2):\n",
      "     Name  Age     City  Score  Passed\n",
      "0  Alice   25    Delhi     85    True\n",
      "1    Bob   30   Mumbai     92    True\n",
      "3  David   40  Kolkata     88    True\n",
      "4    Eve   22  Chennai     91    True \n",
      "\n",
      "Rename 'City' -> 'Location':\n",
      "       Name  Age   Location  Score  Passed\n",
      "0    Alice   25      Delhi     85    True\n",
      "1      Bob   30     Mumbai     92    True\n",
      "2  Charlie   35  Bengaluru     78   False\n",
      "3    David   40    Kolkata     88    True\n",
      "4      Eve   22    Chennai     91    True \n",
      "\n",
      "Set 'Name' as index (new DataFrame):\n",
      "          Age       City  Score  Passed\n",
      "Name                                  \n",
      "Alice     25      Delhi     85    True\n",
      "Bob       30     Mumbai     92    True\n",
      "Charlie   35  Bengaluru     78   False\n",
      "David     40    Kolkata     88    True\n",
      "Eve       22    Chennai     91    True \n",
      "\n",
      "Reset index (back to numeric):\n",
      "       Name  Age       City  Score  Passed\n",
      "0    Alice   25      Delhi     85    True\n",
      "1      Bob   30     Mumbai     92    True\n",
      "2  Charlie   35  Bengaluru     78   False\n",
      "3    David   40    Kolkata     88    True\n",
      "4      Eve   22    Chennai     91    True \n",
      "\n",
      "\n",
      "=== 2) .loc (label-based) ===\n",
      "\n",
      "Row with label/index 1 (as Series):\n",
      " Name         Bob\n",
      "Age           30\n",
      "City      Mumbai\n",
      "Score         92\n",
      "Passed      True\n",
      "Name: 1, dtype: object \n",
      "\n",
      "Rows 1 & 3, columns 'Name' and 'Score':\n",
      "     Name  Score\n",
      "1    Bob     92\n",
      "3  David     88 \n",
      "\n",
      "Rows 1 to 3 (inclusive), columns 'Name' to 'City' (inclusive):\n",
      "       Name  Age       City\n",
      "1      Bob   30     Mumbai\n",
      "2  Charlie   35  Bengaluru\n",
      "3    David   40    Kolkata \n",
      "\n",
      "All rows, columns 'Name' and 'Age':\n",
      "       Name  Age\n",
      "0    Alice   25\n",
      "1      Bob   30\n",
      "2  Charlie   35\n",
      "3    David   40\n",
      "4      Eve   22 \n",
      "\n",
      "Select rows where Age > 30 (loc + boolean mask):\n",
      "       Name  Score\n",
      "2  Charlie     78\n",
      "3    David     88 \n",
      "\n",
      "After updating Alice's Score with loc assignment:\n",
      "       Name  Age       City  Score  Passed\n",
      "0    Alice   25      Delhi     87    True\n",
      "1      Bob   30     Mumbai     92    True\n",
      "2  Charlie   35  Bengaluru     78   False\n",
      "3    David   40    Kolkata     88    True\n",
      "4      Eve   22    Chennai     91    True \n",
      "\n",
      "\n",
      "=== 3) .iloc (position-based) ===\n",
      "\n",
      "First row by position (iloc[0]):\n",
      " Name      Alice\n",
      "Age          25\n",
      "City      Delhi\n",
      "Score        87\n",
      "Passed     True\n",
      "Name: 0, dtype: object \n",
      "\n",
      "df.iloc[1:4, 0:3] -> rows pos 1..3, cols pos 0..2:\n",
      "       Name  Age       City\n",
      "1      Bob   30     Mumbai\n",
      "2  Charlie   35  Bengaluru\n",
      "3    David   40    Kolkata \n",
      "\n",
      "Rows at positions [0,2], columns [1,3] ->\n",
      "    Age  Score\n",
      "0   25     87\n",
      "2   35     78 \n",
      "\n",
      "Last row with iloc[-1]:\n",
      " Name          Eve\n",
      "Age            22\n",
      "City      Chennai\n",
      "Score          91\n",
      "Passed       True\n",
      "Name: 4, dtype: object \n",
      "\n",
      "\n",
      "=== 4) loc vs iloc differences ===\n",
      "\n",
      "df2 (Age as index):\n",
      "         Name       City  Score  Passed\n",
      "Age                                   \n",
      "25     Alice      Delhi     87    True\n",
      "30       Bob     Mumbai     92    True\n",
      "35   Charlie  Bengaluru     78   False\n",
      "40     David    Kolkata     88    True\n",
      "22       Eve    Chennai     91    True \n",
      "\n",
      "df2.loc[30] -> row with index label 30:\n",
      " Name         Bob\n",
      "City      Mumbai\n",
      "Score         92\n",
      "Passed      True\n",
      "Name: 30, dtype: object \n",
      "\n",
      "df2.iloc[1] -> second row by position:\n",
      " Name         Bob\n",
      "City      Mumbai\n",
      "Score         92\n",
      "Passed      True\n",
      "Name: 30, dtype: object \n",
      "\n",
      "\n",
      "=== 5) Boolean indexing ===\n",
      "\n",
      "Mask (Score>=90 & Age<35): [False, True, False, False, True]\n",
      "Filtered rows with mask:\n",
      "   Name  Age     City  Score  Passed\n",
      "1  Bob   30   Mumbai     92    True\n",
      "4  Eve   22  Chennai     91    True \n",
      "\n",
      "Using df.query('Score>=90 and Age < 35'):\n",
      "   Name  Age     City  Score  Passed\n",
      "1  Bob   30   Mumbai     92    True\n",
      "4  Eve   22  Chennai     91    True \n",
      "\n",
      "\n",
      "=== 6) .ix — deprecated (do NOT use) ===\n",
      "\n",
      "Note: .ix was deprecated and removed in pandas >= 0.20. It tried to be 'label or position' ambiguous.\n",
      "Use .loc for label-based selection and .iloc for position-based selection.\n",
      "\n",
      "Use .loc[1,'Name'] if 1 is a label; use .iloc[1,0] if 1 is a position.\n",
      "\n",
      "\n",
      "=== 7) Series string operations (.str.slice) ===\n",
      "\n",
      "Original names:\n",
      " ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\n",
      "First 3 chars (names.str.slice(0,3)):\n",
      " ['Ali', 'Bob', 'Cha', 'Dav', 'Eve'] \n",
      "\n",
      "names.str.upper(): ['ALICE', 'BOB', 'CHARLIE', 'DAVID', 'EVE']\n",
      "names.str.contains('a') -> boolean Series indicating whether 'a' appears:\n",
      " 0    False\n",
      "1    False\n",
      "2     True\n",
      "3     True\n",
      "4    False\n",
      "Name: Name, dtype: bool\n",
      "\n",
      "=== 8) Column slicing ===\n",
      "\n",
      "Columns 'Name' through 'City' (label-range with loc):\n",
      "       Name  Age       City\n",
      "0    Alice   25      Delhi\n",
      "1      Bob   30     Mumbai\n",
      "2  Charlie   35  Bengaluru\n",
      "3    David   40    Kolkata\n",
      "4      Eve   22    Chennai \n",
      "\n",
      "Select columns by list ['City','Score']:\n",
      "         City  Score\n",
      "0      Delhi     87\n",
      "1     Mumbai     92\n",
      "2  Bengaluru     78\n",
      "3    Kolkata     88\n",
      "4    Chennai     91 \n",
      "\n",
      "Columns by positional slice iloc[:, 1:3] ->\n",
      "    Age       City\n",
      "0   25      Delhi\n",
      "1   30     Mumbai\n",
      "2   35  Bengaluru\n",
      "3   40    Kolkata\n",
      "4   22    Chennai \n",
      "\n",
      "Columns with name starting with 'S' using filter(regex):\n",
      " ['Score'] \n",
      "\n",
      "\n",
      "=== 9) Accessing different rows ===\n",
      "\n",
      "df[1:4] -> rows positions 1..3 (slice by position on default index):\n",
      "       Name  Age       City  Score  Passed\n",
      "1      Bob   30     Mumbai     92    True\n",
      "2  Charlie   35  Bengaluru     78   False\n",
      "3    David   40    Kolkata     88    True \n",
      "\n",
      "df.loc[[0,2,4]] -> rows with labels 0,2,4:\n",
      "       Name  Age       City  Score  Passed\n",
      "0    Alice   25      Delhi     87    True\n",
      "2  Charlie   35  Bengaluru     78   False\n",
      "4      Eve   22    Chennai     91    True \n",
      "\n",
      "Rows where City == 'Mumbai' or Score>90:\n",
      "   Name  Age     City  Score  Passed\n",
      "1  Bob   30   Mumbai     92    True\n",
      "4  Eve   22  Chennai     91    True \n",
      "\n",
      "\n",
      "=== 10) Tips & best practices ===\n",
      "\n",
      "- Prefer .loc and .iloc explicitly; they are unambiguous.\n",
      "- Use boolean masks for filtering; combine masks with & and | and wrap conditions with parentheses.\n",
      "- Use df.at[row_label, col_label] or df.iat[row_pos, col_pos] for fast scalar access/assignment.\n",
      "- Avoid chained indexing like df[df['A']>0]['B'] = val (may cause SettingWithCopyWarning). Use loc instead.\n",
      "- When index labels are integers, be careful: loc uses labels, iloc uses positions.\n",
      "\n",
      "=== DONE ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas_indexing_examples.py\n",
    "# Demonstrates rows/columns operations, loc/iloc, boolean indexing, .ix (deprecated note),\n",
    "# Series.str.slice, and column slicing. All examples are top-level with comments.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n=== SETUP: sample DataFrame ===\\n\")\n",
    "\n",
    "# Build a sample DataFrame used across examples\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age' : [25, 30, 35, 40, 22],\n",
    "    'City': ['Delhi', 'Mumbai', 'Bengaluru', 'Kolkata', 'Chennai'],\n",
    "    'Score':[85, 92, 78, 88, 91]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "# Show index and columns (default index 0..n-1)\n",
    "print(\"Initial DataFrame:\\n\", df, \"\\n\")\n",
    "print(\"shape:\", df.shape)\n",
    "print(\"columns:\", df.columns)\n",
    "print(\"index:\", df.index, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) ROWS & COLUMNS: add, drop, rename, set/reset index\n",
    "# ============================================================\n",
    "print(\"\\n=== 1) Rows & Columns operations ===\\n\")\n",
    "\n",
    "# Add a new column (vectorized assignment)\n",
    "# df['NewCol'] = <series-like> ; new column aligned by index\n",
    "df['Passed'] = df['Score'] >= 80   # boolean column\n",
    "print(\"Added 'Passed' column:\\n\", df, \"\\n\")\n",
    "\n",
    "# Drop a column:\n",
    "# df.drop(columns=['colname'], inplace=False) returns new DF by default\n",
    "df_dropped = df.drop(columns=['Passed'])\n",
    "print(\"After df.drop(columns=['Passed']) (copy):\\n\", df_dropped, \"\\n\")\n",
    "\n",
    "# Drop a row by label (index value):\n",
    "# df.drop(index=label) ; axis=0 by default\n",
    "df_droprow = df.drop(index=2)   # removes row with index 2 ('Charlie')\n",
    "print(\"After df.drop(index=2):\\n\", df_droprow, \"\\n\")\n",
    "\n",
    "# Rename columns:\n",
    "# df.rename(columns={'old':'new'}, inplace=False)\n",
    "print(\"Rename 'City' -> 'Location':\\n\", df.rename(columns={'City':'Location'}), \"\\n\")\n",
    "\n",
    "# Set an existing column as index:\n",
    "# df.set_index('Name', inplace=False)\n",
    "df_indexed = df.set_index('Name')\n",
    "print(\"Set 'Name' as index (new DataFrame):\\n\", df_indexed, \"\\n\")\n",
    "\n",
    "# Reset index back to default:\n",
    "print(\"Reset index (back to numeric):\\n\", df_indexed.reset_index(), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) loc — label-based indexing\n",
    "# ============================================================\n",
    "print(\"\\n=== 2) .loc (label-based) ===\\n\")\n",
    "\n",
    "# Basic: df.loc[row_label, col_label]\n",
    "# When index is default integers, row_label is integer index value\n",
    "print(\"Row with label/index 1 (as Series):\\n\", df.loc[1], \"\\n\")\n",
    "\n",
    "# Select multiple rows and columns by labels:\n",
    "# df.loc[[row_labels], [col_labels]]\n",
    "print(\"Rows 1 & 3, columns 'Name' and 'Score':\\n\", df.loc[[1,3], ['Name','Score']], \"\\n\")\n",
    "\n",
    "# Slicing with labels (inclusive of the end label for loc)\n",
    "# df.loc[start_label : end_label, start_col : end_col]\n",
    "print(\"Rows 1 to 3 (inclusive), columns 'Name' to 'City' (inclusive):\\n\",\n",
    "      df.loc[1:3, 'Name':'City'], \"\\n\")\n",
    "\n",
    "# Selecting all rows but specific columns:\n",
    "print(\"All rows, columns 'Name' and 'Age':\\n\", df.loc[:, ['Name','Age']], \"\\n\")\n",
    "\n",
    "# Boolean condition with loc:\n",
    "# df.loc[df['Age'] > 30, ['Name','Score']]\n",
    "print(\"Select rows where Age > 30 (loc + boolean mask):\\n\",\n",
    "      df.loc[df['Age'] > 30, ['Name','Score']], \"\\n\")\n",
    "\n",
    "# Assigning using loc (in-place)\n",
    "# df.loc[mask, 'col'] = value  — modifies DataFrame in-place\n",
    "df.loc[df['Name'] == 'Alice', 'Score'] = 87\n",
    "print(\"After updating Alice's Score with loc assignment:\\n\", df, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) iloc — integer position based indexing\n",
    "# ============================================================\n",
    "print(\"\\n=== 3) .iloc (position-based) ===\\n\")\n",
    "\n",
    "# iloc uses integer positions [row_pos, col_pos], zero-based and end-exclusive for slices\n",
    "print(\"First row by position (iloc[0]):\\n\", df.iloc[0], \"\\n\")\n",
    "\n",
    "# Select rows 1..3 by position (end-exclusive), and columns 0..2\n",
    "print(\"df.iloc[1:4, 0:3] -> rows pos 1..3, cols pos 0..2:\\n\", df.iloc[1:4, 0:3], \"\\n\")\n",
    "\n",
    "# Fancy indexing by positions: pass lists of integer positions\n",
    "print(\"Rows at positions [0,2], columns [1,3] ->\\n\", df.iloc[[0,2],[1,3]], \"\\n\")\n",
    "\n",
    "# Negative indices allowed (like Python lists)\n",
    "print(\"Last row with iloc[-1]:\\n\", df.iloc[-1], \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Combined examples: loc vs iloc differences\n",
    "# ============================================================\n",
    "print(\"\\n=== 4) loc vs iloc differences ===\\n\")\n",
    "\n",
    "# If index labels are integers and non-default, loc treats them as labels (not positions)\n",
    "df2 = df.set_index('Age')   # index now values [25,30,35,40,22]\n",
    "print(\"df2 (Age as index):\\n\", df2, \"\\n\")\n",
    "# df2.loc[30] -> uses label 30 (row where Age==30)\n",
    "print(\"df2.loc[30] -> row with index label 30:\\n\", df2.loc[30], \"\\n\")\n",
    "# df2.iloc[1] -> second row by position\n",
    "print(\"df2.iloc[1] -> second row by position:\\n\", df2.iloc[1], \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Boolean indexing (filtering)\n",
    "# ============================================================\n",
    "print(\"\\n=== 5) Boolean indexing ===\\n\")\n",
    "\n",
    "# Boolean mask example:\n",
    "mask = (df['Score'] >= 90) & (df['Age'] < 35)\n",
    "print(\"Mask (Score>=90 & Age<35):\", mask.tolist())\n",
    "print(\"Filtered rows with mask:\\n\", df[mask], \"\\n\")\n",
    "\n",
    "# Use .query() as a string-based filter alternative (useful for readability)\n",
    "print(\"Using df.query('Score>=90 and Age < 35'):\\n\", df.query('Score >= 90 and Age < 35'), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) .ix (DEPRECATED) — explanation and safe alternative\n",
    "# ============================================================\n",
    "print(\"\\n=== 6) .ix — deprecated (do NOT use) ===\\n\")\n",
    "print(\"Note: .ix was deprecated and removed in pandas >= 0.20. It tried to be 'label or position' ambiguous.\")\n",
    "print(\"Use .loc for label-based selection and .iloc for position-based selection.\\n\")\n",
    "\n",
    "# For historical demonstration only: show recommended replacements\n",
    "# Example intention: \"select row with label 1 and column 'Name'\"\n",
    "print(\"Use .loc[1,'Name'] if 1 is a label; use .iloc[1,0] if 1 is a position.\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Series string slicing (.str.slice and other .str helpers)\n",
    "# ============================================================\n",
    "print(\"\\n=== 7) Series string operations (.str.slice) ===\\n\")\n",
    "\n",
    "# Suppose we want first 3 characters of each Name\n",
    "names = df['Name']\n",
    "print(\"Original names:\\n\", names.tolist())\n",
    "# Series.str.slice(start, stop, step) -> works on string Series\n",
    "# start inclusive, stop exclusive (like Python slicing)\n",
    "print(\"First 3 chars (names.str.slice(0,3)):\\n\", names.str.slice(0, 3).tolist(), \"\\n\")\n",
    "\n",
    "# Other useful .str methods: .lower(), .upper(), .contains(), .split(), .replace()\n",
    "print(\"names.str.upper():\", names.str.upper().tolist())\n",
    "print(\"names.str.contains('a') -> boolean Series indicating whether 'a' appears:\\n\", names.str.contains('a'))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) Column slices: label-range and positional slicing\n",
    "# ============================================================\n",
    "print(\"\\n=== 8) Column slicing ===\\n\")\n",
    "\n",
    "# 8A: Label-range slicing (inclusive) with loc:\n",
    "# df.loc[:, 'Name':'City'] -> selects all rows and columns from 'Name' through 'City' (inclusive)\n",
    "print(\"Columns 'Name' through 'City' (label-range with loc):\\n\", df.loc[:, 'Name':'City'], \"\\n\")\n",
    "\n",
    "# 8B: Column list to select specific columns:\n",
    "print(\"Select columns by list ['City','Score']:\\n\", df[['City','Score']], \"\\n\")\n",
    "\n",
    "# 8C: Positional column slice using iloc:\n",
    "# df.iloc[:, 1:4] -> selects columns by position (end-exclusive)\n",
    "print(\"Columns by positional slice iloc[:, 1:3] ->\\n\", df.iloc[:, 1:3], \"\\n\")\n",
    "\n",
    "# 8D: Using filter to choose columns by regex or like\n",
    "print(\"Columns with name starting with 'S' using filter(regex):\\n\", df.filter(regex='^S').columns.tolist(), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) Accessing multiple rows in different ways (examples)\n",
    "# ============================================================\n",
    "print(\"\\n=== 9) Accessing different rows ===\\n\")\n",
    "\n",
    "# contiguous rows by slice\n",
    "print(\"df[1:4] -> rows positions 1..3 (slice by position on default index):\\n\", df[1:4], \"\\n\")\n",
    "\n",
    "# rows by explicit list of labels (fancy indexing)\n",
    "print(\"df.loc[[0,2,4]] -> rows with labels 0,2,4:\\n\", df.loc[[0,2,4]], \"\\n\")\n",
    "\n",
    "# selecting by boolean masks built from multiple conditions:\n",
    "print(\"Rows where City == 'Mumbai' or Score>90:\\n\", df[(df['City']=='Mumbai') | (df['Score']>90)], \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10) Good practices & tips\n",
    "# ============================================================\n",
    "print(\"\\n=== 10) Tips & best practices ===\\n\")\n",
    "print(\"- Prefer .loc and .iloc explicitly; they are unambiguous.\")\n",
    "print(\"- Use boolean masks for filtering; combine masks with & and | and wrap conditions with parentheses.\")\n",
    "print(\"- Use df.at[row_label, col_label] or df.iat[row_pos, col_pos] for fast scalar access/assignment.\")\n",
    "print(\"- Avoid chained indexing like df[df['A']>0]['B'] = val (may cause SettingWithCopyWarning). Use loc instead.\")\n",
    "print(\"- When index labels are integers, be careful: loc uses labels, iloc uses positions.\\n\")\n",
    "\n",
    "print(\"=== DONE ===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71093e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SETUP SAMPLE DATA ===\n",
      "\n",
      "Sample DataFrame:\n",
      "     A    B  C\n",
      "0  10  1.5  x\n",
      "1  20  2.5  y\n",
      "2  30  3.5  x\n",
      "3  40  NaN  z \n",
      "\n",
      "\n",
      "=== 1) DataFrame.apply examples ===\n",
      "\n",
      "Apply row_sum to each row (axis=1):\n",
      "0    11.5\n",
      "1    22.5\n",
      "2    33.5\n",
      "3    40.0\n",
      "dtype: float64\n",
      "\n",
      "Apply row_stats with result_type='expand' to convert tuples to columns:\n",
      "      0     1\n",
      "0  11.5   8.5\n",
      "1  22.5  17.5\n",
      "2  33.5  26.5\n",
      "3   NaN   NaN\n",
      "\n",
      "Apply add_scalar to each column with args=(5,):\n",
      "    A    B\n",
      "0  15  6.5\n",
      "1  25  7.5\n",
      "2  35  8.5\n",
      "3  45  NaN\n",
      "\n",
      "\n",
      "=== 2) Series.apply examples ===\n",
      "\n",
      "Series.apply with sqrt (elementwise):\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "3    4.0\n",
      "dtype: float64\n",
      "\n",
      "Series.apply with custom function + args (p=3):\n",
      "0       1\n",
      "1      64\n",
      "2     729\n",
      "3    4096\n",
      "dtype: int64\n",
      "\n",
      "Direct NumPy ufunc (faster) - np.sqrt(s):\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "3    4.0\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "=== 3) DataFrame.agg examples ===\n",
      "\n",
      "Column-wise mean and sum using a list of functions:\n",
      "        A     B\n",
      "mean  2.0   5.0\n",
      "sum   6.0  15.0\n",
      "\n",
      "Different aggregations per column using dict:\n",
      "        A    B\n",
      "mean  2.0  5.0\n",
      "min   1.0  NaN\n",
      "std   NaN  1.0\n",
      "\n",
      "Named-style aggregation producing a flat column MultiIndex (example):\n",
      "          A     B\n",
      "A_mean  2.0   NaN\n",
      "B_sum   NaN  15.0\n",
      "\n",
      "2010-12-31 08:45:00    11.0\n",
      "2011-12-31 08:45:00    21.0\n",
      "2012-12-31 08:45:00     8.0\n",
      "2013-12-31 08:45:00    18.0\n",
      "2014-12-31 08:45:00    65.0\n",
      "2015-12-31 08:45:00    18.0\n",
      "2016-12-31 08:45:00    32.0\n",
      "2017-12-31 08:45:00    10.0\n",
      "2018-12-31 08:45:00     5.0\n",
      "2019-12-31 08:45:00    32.0\n",
      "2020-12-31 08:45:00     NaN\n",
      "Freq: YE-DEC, dtype: float64\n",
      "\n",
      "=== 4) mean() examples ===\n",
      "\n",
      "Mean of column A: 2.0\n",
      "Mean across columns for each row (numeric only):\n",
      "0    2.5\n",
      "1    3.5\n",
      "2    4.5\n",
      "dtype: float64\n",
      "\n",
      "=== 5) MAD (mean absolute deviation) examples ===\n",
      "\n",
      "Manual MAD for s2: 1.7600000000000002\n",
      "Equivalent expression: (s - s.mean()).abs().mean()\n",
      "\n",
      "\n",
      "=== 6) sem() examples ===\n",
      "\n",
      "Standard error of column A (Series.sem): 0.5773502691896258\n",
      "Standard error across rows (DataFrame.sem axis=1):\n",
      " 0    1.5\n",
      "1    1.5\n",
      "2    1.5\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "=== 7) value_counts() examples ===\n",
      "\n",
      "Value counts for column C:\n",
      "C\n",
      "7    1\n",
      "8    1\n",
      "9    1\n",
      "Name: count, dtype: int64\n",
      "Relative frequencies (normalize=True):\n",
      "C\n",
      "7    0.333333\n",
      "8    0.333333\n",
      "9    0.333333\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "\n",
      "=== 8) apply with lambda and applymap ===\n",
      "\n",
      "After row-wise lambda (A + B with NaN safe):\n",
      "    A  B  A_plus_B\n",
      "0  1  4         5\n",
      "1  2  5         7\n",
      "2  3  6         9 \n",
      "\n",
      "Elementwise applymap example (numeric -> 'v=...'):\n",
      "     A    B\n",
      "0  v=1  v=4\n",
      "1  v=2  v=5\n",
      "2  v=3  v=6\n",
      "\n",
      "\n",
      "=== 9) Performance tips ===\n",
      "\n",
      "- Prefer vectorized pandas/NumPy operations for speed.\n",
      "- Use apply/agg when you need custom Python-level logic.\n",
      "- For elementwise scalar transformations use Series.map/.apply for clarity (but still Python-level).\n",
      "\n",
      "=== DONE: pandas apply/agg examples ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_22376\\65394896.py:134: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  index_ = pd.date_range('2010-10-09 08:45', periods = 11, freq ='Y')\n",
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_22376\\65394896.py:219: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  print(df[['A','B']].applymap(prefix_v))\n"
     ]
    }
   ],
   "source": [
    "# pandas_apply_agg_examples.py\n",
    "# Demonstrates: DataFrame.apply, Series.apply, DataFrame.agg, mean, mad (manual), sem, value_counts,\n",
    "# apply with lambda (row-wise), applymap (elementwise), and usage notes.\n",
    "#\n",
    "# Run: python pandas_apply_agg_examples.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n=== SETUP SAMPLE DATA ===\\n\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'A': [10, 20, 30, 40],\n",
    "    'B': [1.5, 2.5, 3.5, np.nan],\n",
    "    'C': ['x', 'y', 'x', 'z']\n",
    "})\n",
    "\n",
    "print(\"Sample DataFrame:\\n\", df, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) DataFrame.apply(func, axis=0, raw=False, result_type=None, args=(), **kwargs)\n",
    "# - Applies function along an axis (0 = index/columns, 1 = columns/rows)\n",
    "# - If func returns a Series for each input, results can be combined into a DataFrame.\n",
    "# - result_type controls how list-like results are combined (None, 'expand', 'reduce', 'broadcast').\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== 1) DataFrame.apply examples ===\\n\")\n",
    "\n",
    "# Example B: apply function to each ROW (axis=1)\n",
    "# Here the function receives a Series for the row (index = column names)\n",
    "def row_sum(row):\n",
    "    # sum numeric columns in the row (skip non-numeric automatically by pandas' sum)\n",
    "    return row[['A', 'B']].sum()\n",
    "\n",
    "print(\"Apply row_sum to each row (axis=1):\")\n",
    "print(df.apply(row_sum, axis=1))\n",
    "print()\n",
    "\n",
    "# Example C: result_type='expand' when func returns a sequence for each row\n",
    "# result_type='expand' will expand sequences into columns (DataFrame)\n",
    "def row_stats(row):\n",
    "    a = row['A']\n",
    "    b = row['B']\n",
    "    return (a + b, a - b)   # 2-tuple → will expand\n",
    "\n",
    "print(\"Apply row_stats with result_type='expand' to convert tuples to columns:\")\n",
    "print(df.apply(row_stats, axis=1, result_type='expand'))\n",
    "print()\n",
    "\n",
    "# Example D: passing additional args via args=()\n",
    "def add_scalar(col, scalar):\n",
    "    \"\"\"Add scalar to column (works when apply passes Series col).\"\"\"\n",
    "    return col + scalar\n",
    "\n",
    "print(\"Apply add_scalar to each column with args=(5,):\")\n",
    "print(df[['A','B']].apply(add_scalar, args=(5,)))   # only numeric columns shown for clarity\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Series.apply(func, convert_dtype=True, args=(), **kwargs)\n",
    "# - Applies function elementwise to Series values (or a ufunc to whole Series).\n",
    "# - convert_dtype: try to coerce result to a better dtype (default True).\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== 2) Series.apply examples ===\\n\")\n",
    "\n",
    "s = pd.Series([1, 4, 9, 16])\n",
    "\n",
    "# Apply sqrt elementwise (function receives scalar)\n",
    "print(\"Series.apply with sqrt (elementwise):\")\n",
    "print(s.apply(np.sqrt))   # equivalent to s.map(np.sqrt) for elementwise\n",
    "print()\n",
    "\n",
    "# Using a Python function with args\n",
    "def power(x, p=2):\n",
    "    return x ** p\n",
    "\n",
    "print(\"Series.apply with custom function + args (p=3):\")\n",
    "print(s.apply(power, p=3))   # passing keyword arg works too (via kwargs)\n",
    "print()\n",
    "\n",
    "# Note: for vectorized NumPy ufuncs prefer calling ufunc on Series directly (faster):\n",
    "print(\"Direct NumPy ufunc (faster) - np.sqrt(s):\")\n",
    "print(np.sqrt(s))\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Using.values()\n",
    "# ============================================================\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\n",
    "df['add'] = np.sum(df[['A', 'B', 'C']].values, axis=1)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) DataFrame.agg / agg (aggregate) — flexible aggregations\n",
    "# - Accepts function name string, function, list of functions, or dict mapping column->functions\n",
    "# - Aggregation is performed over an axis (default axis=0 meaning aggregate each column)\n",
    "\n",
    "# df.select_dtypes(include='number').aggregate(['sum', 'min'])\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== 3) DataFrame.agg examples ===\\n\")\n",
    "\n",
    "print(\"Column-wise mean and sum using a list of functions:\")\n",
    "print(df[['A','B']].agg(['mean', 'sum']))    # returns DataFrame with rows = agg names, cols = original columns\n",
    "print()\n",
    "\n",
    "print(\"Different aggregations per column using dict:\")\n",
    "print(df.agg({'A': ['mean', 'min'], 'B': ['mean', 'std']}))\n",
    "print()\n",
    "\n",
    "# Apply a named aggregation (useful with groupby too) — here on full DataFrame\n",
    "print(\"Named-style aggregation producing a flat column MultiIndex (example):\")\n",
    "print(df.agg(A_mean=('A', 'mean'), B_sum=('B', 'sum')))\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# pd.date_range()\n",
    "# ============================================================\n",
    "\n",
    "# importing pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Creating the Series\n",
    "sr = pd.Series([11, 21, 8, 18, 65, 18, 32, 10, 5, 32, None])\n",
    "\n",
    "# Create the Index\n",
    "# apply yearly frequency\n",
    "index_ = pd.date_range('2010-10-09 08:45', periods = 11, freq ='Y')\n",
    "\n",
    "# set the index\n",
    "sr.index = index_\n",
    "\n",
    "# Print the series\n",
    "print(sr)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) mean() — Series.mean() and DataFrame.mean()\n",
    "# - Parameters: axis, skipna (default True), numeric_only, level, etc.\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== 4) mean() examples ===\\n\")\n",
    "\n",
    "print(\"Mean of column A:\", df['A'].mean())\n",
    "print(\"Mean across columns for each row (numeric only):\")\n",
    "print(df[['A','B']].mean(axis=1))   # axis=1 computes per-row mean (over columns)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) mad() — mean absolute deviation\n",
    "# - NOTE: pandas.DataFrame.mad and Series.mad are deprecated in recent pandas versions.\n",
    "# - Manual equivalent: (s - s.mean()).abs().mean()\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== 5) MAD (mean absolute deviation) examples ===\\n\")\n",
    "\n",
    "s2 = pd.Series([2.0, 4.0, 4.0, 6.0, 8.0])\n",
    "\n",
    "# Manual MAD (preferred since mad() deprecation)\n",
    "mad_manual = (s2 - s2.mean()).abs().mean()\n",
    "print(\"Manual MAD for s2:\", mad_manual)\n",
    "\n",
    "# If mad() is available in your pandas version you could call s2.mad(), but prefer manual form for compatibility.\n",
    "print(\"Equivalent expression: (s - s.mean()).abs().mean()\")\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) sem() — standard error of the mean\n",
    "# - DataFrame.sem() / Series.sem()\n",
    "# - Parameters include: axis, skipna=True, ddof=1 (delta degrees of freedom), numeric_only\n",
    "# - sem = std / sqrt(N) (with ddof affecting std)\n",
    "# ============================================================\n",
    "print(\"\\n=== 6) sem() examples ===\\n\")\n",
    "\n",
    "print(\"Standard error of column A (Series.sem):\", df['A'].sem())   # ddof default 1\n",
    "print(\"Standard error across rows (DataFrame.sem axis=1):\\n\", df[['A','B']].sem(axis=1))\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Series.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True)\n",
    "# - Returns counts of unique values as a Series (descending by count by default)\n",
    "# ============================================================\n",
    "print(\"\\n=== 7) value_counts() examples ===\\n\")\n",
    "\n",
    "print(\"Value counts for column C:\")\n",
    "print(df['C'].value_counts())   # counts of 'x','y','z'\n",
    "print(\"Relative frequencies (normalize=True):\")\n",
    "print(df['C'].value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) apply with lambda across rows/columns + applymap (elementwise)\n",
    "# - Use df.apply(lambda row: ..., axis=1) for row-wise single-row functions\n",
    "# - Use df.applymap(func) to apply elementwise to each entry of DataFrame\n",
    "# ============================================================\n",
    "print(\"\\n=== 8) apply with lambda and applymap ===\\n\")\n",
    "\n",
    "# Row-wise: create summary column by combining A and B\n",
    "df['A_plus_B'] = df.apply(lambda r: (r['A'] + (r['B'] if pd.notna(r['B']) else 0)), axis=1)\n",
    "print(\"After row-wise lambda (A + B with NaN safe):\\n\", df[['A','B','A_plus_B']], \"\\n\")\n",
    "\n",
    "# Elementwise via applymap: convert numeric cells to strings with 'v=' prefix\n",
    "def prefix_v(x):\n",
    "    # apply only to numeric-ish values; leave strings as-is\n",
    "    if isinstance(x, (int, float, np.integer, np.floating)):\n",
    "        return f\"v={x}\"\n",
    "    return x\n",
    "\n",
    "print(\"Elementwise applymap example (numeric -> 'v=...'):\")\n",
    "print(df[['A','B']].applymap(prefix_v))\n",
    "print()\n",
    "\n",
    "# NOTE: prefer vectorized operations (df['A'] + df['B']) for speed where possible instead of row-wise apply.\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) Performance notes and tips (short)\n",
    "# - Series.apply and DataFrame.apply can call Python code per-element/row/col -> slower than vectorized ops\n",
    "# - Use NumPy ufuncs directly on Series when possible (np.sqrt(series)), or vectorized pandas ops (df['A'] + 5)\n",
    "# - Use apply/agg for flexible transformations/aggregations when vectorized ops are not available\n",
    "# ============================================================\n",
    "print(\"\\n=== 9) Performance tips ===\\n\")\n",
    "print(\"- Prefer vectorized pandas/NumPy operations for speed.\")\n",
    "print(\"- Use apply/agg when you need custom Python-level logic.\")\n",
    "print(\"- For elementwise scalar transformations use Series.map/.apply for clarity (but still Python-level).\")\n",
    "print()\n",
    "\n",
    "print(\"=== DONE: pandas apply/agg examples ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd796c",
   "metadata": {},
   "source": [
    "Function\tDescription\n",
    "\n",
    "DataFrame.iat[]\tAccess a single value for a row/column pair by integer position.\n",
    "\n",
    "DataFrame.pop()\tReturn item and drop from DataFrame.\n",
    "\n",
    "DataFrame.xs() Return a cross-section (row(s) or column(s)) from the DataFrame.\n",
    "\n",
    "DataFrame.get()\tGet item from object for given key (e.g DataFrame column).\n",
    "\n",
    "DataFrame.isin()\tReturn a boolean DataFrame showing whether each element is contained in values.\n",
    "\n",
    "DataFrame.where()\tReturn an object of the same shape with entries from self where cond is True otherwise from other.\n",
    "\n",
    "DataFrame.mask()\tReturn an object of the same shape with entries from self where cond is False otherwise from other.\n",
    "\n",
    "DataFrame.insert()\tInsert a column into DataFrame at a specified location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34539b83",
   "metadata": {},
   "source": [
    "================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52de45",
   "metadata": {},
   "source": [
    "In Python Pandas, we have the freedom to add different functions whenever needed like lambda function, sort function, etc. We can apply a lambda function to both the columns and rows of the Pandas data frame.\n",
    "\n",
    "Syntax: lambda arguments: expression\n",
    "\n",
    "An anonymous function which we can pass in instantly without defining a name or any thing like a full traditional function.\n",
    "\n",
    "Applying Lambda Functions to Pandas\n",
    "Below are some methods and ways by which we can apply lambda functions to Pandas:\n",
    "\n",
    "Dataframe.assign() on a Single Column\n",
    "Dataframe.assign() on Multiple Columns\n",
    "Dataframe.apply() on a Single Row\n",
    "Dataframe.apply() on Multiple Rows\n",
    "Lambda Function on Multiple Rows and Columns Simultaneously\n",
    "Dataframe.assign() on a Single Column\n",
    "In this example, we will apply the lambda function Dataframe.assign() to a single column. The function is applied to the 'Total_Marks' column, and a new column 'Percentage' is formed with its help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8017990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Total_Marks</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rohan</td>\n",
       "      <td>455</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elvish</td>\n",
       "      <td>250</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deepak</td>\n",
       "      <td>495</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Soni</td>\n",
       "      <td>400</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radhika</td>\n",
       "      <td>350</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vansh</td>\n",
       "      <td>450</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name  Total_Marks  Percentage\n",
       "0    Rohan          455        91.0\n",
       "1   Elvish          250        50.0\n",
       "2   Deepak          495        99.0\n",
       "3     Soni          400        80.0\n",
       "4  Radhika          350        70.0\n",
       "5    Vansh          450        90.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing pandas library\n",
    "import pandas as pd\n",
    " \n",
    "# creating and initializing a list\n",
    "values= [['Rohan',455],['Elvish',250],['Deepak',495],\n",
    "         ['Soni',400],['Radhika',350],['Vansh',450]] \n",
    "\n",
    "# creating a pandas dataframe\n",
    "df = pd.DataFrame(values,columns=['Name','Total_Marks'])\n",
    "\n",
    "# Applying lambda function to find \n",
    "# percentage of 'Total_Marks' column \n",
    "# using df.assign()\n",
    "df = df.assign(Percentage = lambda x: (x['Total_Marks'] /500 * 100))\n",
    "\n",
    "# displaying the data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdc26848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Field_1  Field_2  Field_3\n",
      "a       15      2.5      100\n",
      "b       20      4.5       50\n",
      "c       25      5.2       80\n",
      "d       45      5.8       48\n",
      "e       40      6.3       70\n",
      "f       41      6.4       90\n",
      "g       51      2.3      111\n",
      "   Field_1  Field_2  Field_3\n",
      "a     15.0     2.50    100.0\n",
      "b     20.0     4.50     50.0\n",
      "c     25.0     5.20     80.0\n",
      "d   2025.0    33.64   2304.0\n",
      "e     40.0     6.30     70.0\n",
      "f     41.0     6.40     90.0\n",
      "g     51.0     2.30    111.0\n"
     ]
    }
   ],
   "source": [
    "# importing pandas and numpy libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# creating and initializing a nested list\n",
    "values_list = [[15, 2.5, 100], [20, 4.5, 50], [25, 5.2, 80],\n",
    "               [45, 5.8, 48], [40, 6.3, 70], [41, 6.4, 90], \n",
    "               [51, 2.3, 111]]\n",
    "\n",
    "# creating a pandas dataframe\n",
    "df = pd.DataFrame(values_list, columns=['Field_1', 'Field_2', 'Field_3'],\n",
    "                  index=['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n",
    "\n",
    "print(df)\n",
    "# Apply function numpy.square() to square\n",
    "# the values of one row only i.e. row \n",
    "# with index name 'd'\n",
    "df = df.apply(lambda x: np.square(x) if x.name == 'd' else x, axis=1)\n",
    "\n",
    "\n",
    "# printing dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302427e7",
   "metadata": {},
   "source": [
    "## Data Manipulation and Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e72c6",
   "metadata": {},
   "source": [
    "1. https://www.geeksforgeeks.org/pandas/adding-new-column-to-existing-dataframe-in-pandas/\n",
    "\n",
    "2. https://www.geeksforgeeks.org/python/python-delete-rows-columns-from-dataframe-using-pandas-drop/\n",
    "\n",
    "3. https://www.geeksforgeeks.org/python/python-pandas-dataframe-truncate/\n",
    "\n",
    "4. https://www.geeksforgeeks.org/pandas/python-pandas-series-truncate/\n",
    "\n",
    "5. https://www.geeksforgeeks.org/data-analysis/iterating-over-rows-and-columns-in-pandas-dataframe/\n",
    "\n",
    "6. https://www.geeksforgeeks.org/pandas/python-pandas-dataframe-sort_values-set-1/\n",
    "\n",
    "7. https://www.geeksforgeeks.org/python/python-pandas-dataframe-sort_values-set-2/\n",
    "\n",
    "8. https://www.geeksforgeeks.org/pandas/how-to-add-one-row-in-an-existing-pandas-dataframe/\n",
    "\n",
    "9. https://www.geeksforgeeks.org/pandas/pandas-groupby/\n",
    "\n",
    "10. https://www.geeksforgeeks.org/python/grouping-rows-in-pandas/\n",
    "\n",
    "11. https://www.geeksforgeeks.org/pandas/combining-multiple-columns-in-pandas-groupby-with-dictionary/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c39ef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SETUP: sample DataFrame ===\n",
      "\n",
      "Original DF:\n",
      "     Dept     Name  Age  Salary  Months\n",
      "0     HR    Alice   25   50000      12\n",
      "1     HR      Bob   30   52000      10\n",
      "2    Eng  Charlie   28   80000       8\n",
      "3    Eng    David   40   95000      20\n",
      "4  Sales      Eve   35   60000       6\n",
      "5  Sales    Frank   29   62000       9 \n",
      "\n",
      "=== 1) ADD NEW COLUMNS ===\n",
      "\n",
      "After adding scalar column 'Country':\n",
      "     Dept     Name  Age  Salary  Months Country\n",
      "0     HR    Alice   25   50000      12   India\n",
      "1     HR      Bob   30   52000      10   India\n",
      "2    Eng  Charlie   28   80000       8   India\n",
      "3    Eng    David   40   95000      20   India\n",
      "4  Sales      Eve   35   60000       6   India\n",
      "5  Sales    Frank   29   62000       9   India \n",
      "\n",
      "Added 'Monthly_Est' computed from Salary:\n",
      "       Name  Salary  Monthly_Est\n",
      "0    Alice   50000  4166.666667\n",
      "1      Bob   52000  4333.333333\n",
      "2  Charlie   80000  6666.666667\n",
      "3    David   95000  7916.666667\n",
      "4      Eve   60000  5000.000000\n",
      "5    Frank   62000  5166.666667 \n",
      "\n",
      "Using assign() to add 'Salary_k' (thousands):\n",
      "       Name  Salary_k\n",
      "0    Alice      50.0\n",
      "1      Bob      52.0\n",
      "2  Charlie      80.0\n",
      "3    David      95.0\n",
      "4      Eve      60.0 \n",
      "\n",
      "After df.insert(..., 'Seniority', ...):\n",
      "     Dept     Name Seniority  Age  Salary  Months Country  Monthly_Est\n",
      "0     HR    Alice        Jr   25   50000      12   India  4166.666667\n",
      "1     HR      Bob        Jr   30   52000      10   India  4333.333333\n",
      "2    Eng  Charlie       Mid   28   80000       8   India  6666.666667\n",
      "3    Eng    David        Sr   40   95000      20   India  7916.666667\n",
      "4  Sales      Eve       Mid   35   60000       6   India  5000.000000 \n",
      "\n",
      "=== 2) DROP ROWS / COLUMNS ===\n",
      "\n",
      "Dropped 'Months' column (copy):\n",
      " ['Dept', 'Name', 'Seniority', 'Age', 'Salary', 'Country', 'Monthly_Est'] \n",
      "\n",
      "Drop row index 1 (Bob):\n",
      "     Dept     Name Seniority  Age  Salary  Months Country  Monthly_Est\n",
      "0     HR    Alice        Jr   25   50000      12   India  4166.666667\n",
      "2    Eng  Charlie       Mid   28   80000       8   India  6666.666667\n",
      "3    Eng    David        Sr   40   95000      20   India  7916.666667\n",
      "4  Sales      Eve       Mid   35   60000       6   India  5000.000000\n",
      "5  Sales    Frank       Mid   29   62000       9   India  5166.666667 \n",
      "\n",
      "Drop rows 0 and 4, and column 'Country':\n",
      "     Dept     Name Seniority  Age  Salary  Months  Monthly_Est\n",
      "1     HR      Bob        Jr   30   52000      10  4333.333333\n",
      "2    Eng  Charlie       Mid   28   80000       8  6666.666667\n",
      "3    Eng    David        Sr   40   95000      20  7916.666667\n",
      "5  Sales    Frank       Mid   29   62000       9  5166.666667 \n",
      "\n",
      "=== 3) TRUNCATE ===\n",
      "\n",
      "Indexed by Name:\n",
      "           Dept Seniority  Age  Salary  Months Country  Monthly_Est\n",
      "Name                                                              \n",
      "Alice       HR        Jr   25   50000      12   India  4166.666667\n",
      "Bob         HR        Jr   30   52000      10   India  4333.333333\n",
      "Charlie    Eng       Mid   28   80000       8   India  6666.666667\n",
      "David      Eng        Sr   40   95000      20   India  7916.666667\n",
      "Eve      Sales       Mid   35   60000       6   India  5000.000000\n",
      "Frank    Sales       Mid   29   62000       9   India  5166.666667 \n",
      "\n",
      "df_idx.truncate(before='Bob', after='Eve'):\n",
      "           Dept Seniority  Age  Salary  Months Country  Monthly_Est\n",
      "Name                                                              \n",
      "Bob         HR        Jr   30   52000      10   India  4333.333333\n",
      "Charlie    Eng       Mid   28   80000       8   India  6666.666667\n",
      "David      Eng        Sr   40   95000      20   India  7916.666667\n",
      "Eve      Sales       Mid   35   60000       6   India  5000.000000 \n",
      "\n",
      "Series truncate example (R2..R5):\n",
      " R2    2\n",
      "R3    3\n",
      "R4    4\n",
      "R5    5\n",
      "dtype: int64 \n",
      "\n",
      "=== 4) ITERATING ROWS & COLUMNS ===\n",
      "\n",
      "Using iterrows() (not recommended for heavy loops):\n",
      "index: 0 Name: Alice Salary: 50000\n",
      "index: 1 Name: Bob Salary: 52000\n",
      "index: 2 Name: Charlie Salary: 80000\n",
      "\n",
      "Using itertuples() (faster):\n",
      "Emp(Index=0, Dept='HR', Name='Alice', Seniority='Jr', Age=25, Salary=50000, Months=12, Country='India', Monthly_Est=4166.666666666667)\n",
      "Emp(Index=1, Dept='HR', Name='Bob', Seniority='Jr', Age=30, Salary=52000, Months=10, Country='India', Monthly_Est=4333.333333333333)\n",
      "Emp(Index=2, Dept='Eng', Name='Charlie', Seniority='Mid', Age=28, Salary=80000, Months=8, Country='India', Monthly_Est=6666.666666666667)\n",
      "\n",
      "Iterating columns (column names):\n",
      "col: Dept dtype: object\n",
      "col: Name dtype: object\n",
      "col: Seniority dtype: object\n",
      "col: Age dtype: int64\n",
      "col: Salary dtype: int64\n",
      "col: Months dtype: int64\n",
      "col: Country dtype: object\n",
      "col: Monthly_Est dtype: float64\n",
      "\n",
      "=== 5) SORT_VALUES ===\n",
      "\n",
      "Sort by Age ascending:\n",
      "       Name  Age\n",
      "0    Alice   25\n",
      "2  Charlie   28\n",
      "5    Frank   29\n",
      "1      Bob   30\n",
      "4      Eve   35\n",
      "3    David   40 \n",
      "\n",
      "Sort by Salary descending:\n",
      "       Name  Salary\n",
      "3    David   95000\n",
      "2  Charlie   80000\n",
      "5    Frank   62000\n",
      "4      Eve   60000\n",
      "1      Bob   52000\n",
      "0    Alice   50000 \n",
      "\n",
      "Sort by Dept ascending, Salary descending:\n",
      "     Dept     Name  Salary\n",
      "3    Eng    David   95000\n",
      "2    Eng  Charlie   80000\n",
      "1     HR      Bob   52000\n",
      "0     HR    Alice   50000\n",
      "5  Sales    Frank   62000\n",
      "4  Sales      Eve   60000 \n",
      "\n",
      "Sort with NaN salary (na_position='first'):\n",
      "       Name   Salary\n",
      "2  Charlie      NaN\n",
      "0    Alice  50000.0\n",
      "1      Bob  52000.0\n",
      "4      Eve  60000.0\n",
      "5    Frank  62000.0\n",
      "3    David  95000.0 \n",
      "\n",
      "=== 6) ADD ONE ROW ===\n",
      "\n",
      "After pd.concat to add one row:\n",
      "     Dept   Name Seniority  Age  Salary  Months Country  Monthly_Est\n",
      "4  Sales    Eve       Mid   35   60000       6   India  5000.000000\n",
      "5  Sales  Frank       Mid   29   62000       9   India  5166.666667\n",
      "6    Eng  Grace        Jr   27   70000       7   India  5833.333333 \n",
      "\n",
      "After df.loc[...] = ... (added one row):\n",
      "     Dept   Name Seniority    Age  Salary Months Country  Monthly_Est\n",
      "4  Sales    Eve       Mid     35   60000      6   India  5000.000000\n",
      "5  Sales  Frank       Mid     29   62000      9   India  5166.666667\n",
      "6     QA   Hank        31  55000      11  India     Mid  4583.333333 \n",
      "\n",
      "=== 7) GROUPBY BASIC ===\n",
      "\n",
      "Group keys: ['Eng', 'HR', 'Sales']\n",
      "\n",
      "Group sizes (number of rows per Dept):\n",
      " Dept\n",
      "Eng      2\n",
      "HR       2\n",
      "Sales    2\n",
      "dtype: int64 \n",
      "\n",
      "Group mean of numeric columns:\n",
      "         Age   Salary  Months  Monthly_Est\n",
      "Dept                                     \n",
      "Eng    34.0  87500.0    14.0  7291.666667\n",
      "HR     27.5  51000.0    11.0  4250.000000\n",
      "Sales  32.0  61000.0     7.5  5083.333333 \n",
      "\n",
      "Group sum of Salary:\n",
      " Dept\n",
      "Eng      175000\n",
      "HR       102000\n",
      "Sales    122000\n",
      "Name: Salary, dtype: int64 \n",
      "\n",
      "=== 8) GROUPING ROWS EXAMPLES ===\n",
      "\n",
      "Group sizes by Dept and Seniority:\n",
      " Dept   Seniority\n",
      "Eng    Mid          1\n",
      "       Sr           1\n",
      "HR     Jr           2\n",
      "Sales  Mid          2\n",
      "dtype: int64 \n",
      "\n",
      "Dept: Eng -> rows:\n",
      "    Name  Salary\n",
      "Charlie   80000\n",
      "  David   95000\n",
      "Dept: HR -> rows:\n",
      "  Name  Salary\n",
      "Alice   50000\n",
      "  Bob   52000\n",
      "Dept: Sales -> rows:\n",
      "  Name  Salary\n",
      "  Eve   60000\n",
      "Frank   62000\n",
      "\n",
      "Group 'Eng' rows:\n",
      "   Dept     Name Seniority  Age  Salary  Months Country  Monthly_Est\n",
      "2  Eng  Charlie       Mid   28   80000       8   India  6666.666667\n",
      "3  Eng    David        Sr   40   95000      20   India  7916.666667 \n",
      "\n",
      "=== 9) GROUPBY with dict aggregations (multi-agg per column) ===\n",
      "\n",
      "Agg result with MultiIndex columns:\n",
      "         Salary                  Age Months      \n",
      "          mean     sum    max  mean    sum count\n",
      "Dept                                            \n",
      "Eng    87500.0  175000  95000  34.0     28     2\n",
      "HR     51000.0  102000  52000  27.5     22     2\n",
      "Sales  61000.0  122000  62000  32.0     15     2 \n",
      "\n",
      "Named aggregation (flat columns):\n",
      "        avg_salary  total_salary  max_salary  headcount  avg_months\n",
      "Dept                                                              \n",
      "Eng       87500.0        175000       95000          2        14.0\n",
      "HR        51000.0        102000       52000          2        11.0\n",
      "Sales     61000.0        122000       62000          2         7.5 \n",
      "\n",
      "=== 10) Groupby helpers & tips ===\n",
      "\n",
      "- g.size()     : number of rows per group\n",
      "- g.count()    : non-NA counts per column per group\n",
      "- g.describe() : descriptive stats per group\n",
      "- g.agg(...)   : flexible aggregation (strings, callables, lists, dict)\n",
      "- Use as_index=False in groupby to keep group keys as columns instead of index\n",
      "\n",
      "groupby with as_index=False result:\n",
      "     Dept   Salary\n",
      "0    Eng  87500.0\n",
      "1     HR  51000.0\n",
      "2  Sales  61000.0 \n",
      "\n",
      "=== DONE: rows/cols/groupby examples ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas_rows_cols_groupby_examples.py\n",
    "# Demonstrates: add/drop/truncate/iterate/sort/add-row/groupby and multi-column aggregations.\n",
    "# All examples are top-level, well-spaced, and include short explanatory comments.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n=== SETUP: sample DataFrame ===\\n\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Dept' : ['HR', 'HR', 'Eng', 'Eng', 'Sales', 'Sales'],\n",
    "    'Name' : ['Alice','Bob','Charlie','David','Eve','Frank'],\n",
    "    'Age'  : [25, 30, 28, 40, 35, 29],\n",
    "    'Salary':[50000, 52000, 80000, 95000, 60000, 62000],\n",
    "    'Months':[12, 10, 8, 20, 6, 9]\n",
    "})\n",
    "print(\"Original DF:\\n\", df, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 1) ADDING NEW COLUMN(S)\n",
    "# ============================\n",
    "# Methods:\n",
    "#  - vectorized assignment: df['new'] = <scalar|Series|array>\n",
    "#  - assign(): returns new DataFrame (chainable)\n",
    "#  - insert(): insert at specific column position\n",
    "\n",
    "print(\"=== 1) ADD NEW COLUMNS ===\\n\")\n",
    "\n",
    "# Add column with scalar (same value for all rows)\n",
    "df['Country'] = 'India'   # simple assignment\n",
    "print(\"After adding scalar column 'Country':\\n\", df, \"\\n\")\n",
    "\n",
    "# Add column using vectorized expression (based on other columns)\n",
    "# e.g., monthly salary estimate = Salary / 12\n",
    "df['Monthly_Est'] = df['Salary'] / 12.0\n",
    "print(\"Added 'Monthly_Est' computed from Salary:\\n\", df[['Name','Salary','Monthly_Est']], \"\\n\")\n",
    "\n",
    "# Using assign() (does NOT modify in-place unless assigned)\n",
    "df2 = df.assign(Salary_k = df['Salary'] / 1000.0)   # returns new DF with extra column\n",
    "print(\"Using assign() to add 'Salary_k' (thousands):\\n\", df2[['Name','Salary_k']].head(), \"\\n\")\n",
    "\n",
    "# Insert column at specific position: insert(loc, column, value)\n",
    "df.insert(2, 'Seniority', ['Jr','Jr','Mid','Sr','Mid','Mid'])  # insert as 3rd column (0-based)\n",
    "print(\"After df.insert(..., 'Seniority', ...):\\n\", df.head(), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 2) DROP ROWS / COLUMNS\n",
    "# ============================\n",
    "# df.drop(labels=None, axis=0, index=None, columns=None, inplace=False)\n",
    "# - axis=0 drop rows, axis=1 drop columns\n",
    "# - inplace=False returns a new DF; inplace=True modifies original (not recommended)\n",
    "print(\"=== 2) DROP ROWS / COLUMNS ===\\n\")\n",
    "\n",
    "# Drop a column by name (returns a copy)\n",
    "dropped = df.drop(columns=['Months'])\n",
    "print(\"Dropped 'Months' column (copy):\\n\", dropped.columns.tolist(), \"\\n\")\n",
    "\n",
    "# Drop a row by index label (here default index 0..n-1)\n",
    "df_droprow = df.drop(index=1)   # drop row with index 1 (Bob)\n",
    "print(\"Drop row index 1 (Bob):\\n\", df_droprow, \"\\n\")\n",
    "\n",
    "# Drop multiple rows / columns\n",
    "df_drop_multi = df.drop(index=[0,4], columns=['Country'])\n",
    "print(\"Drop rows 0 and 4, and column 'Country':\\n\", df_drop_multi, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 3) TRUNCATE (DataFrame & Series)\n",
    "# ============================\n",
    "# df.truncate(before=None, after=None, axis=None, copy=True)\n",
    "# - cuts off the DataFrame before/after given index labels (inclusive)\n",
    "# - useful when working with a labeled index (time series); with default integer index it works too\n",
    "print(\"=== 3) TRUNCATE ===\\n\")\n",
    "\n",
    "# For demonstration, set index to Name so truncate uses labels\n",
    "df_idx = df.set_index('Name')\n",
    "print(\"Indexed by Name:\\n\", df_idx, \"\\n\")\n",
    "# Truncate between 'Bob' and 'Eve' inclusive (label-based)\n",
    "trunc = df_idx.truncate(before='Bob', after='Eve')   # includes Bob..Eve\n",
    "print(\"df_idx.truncate(before='Bob', after='Eve'):\\n\", trunc, \"\\n\")\n",
    "\n",
    "# Series.truncate works similarly on a Series:\n",
    "s = pd.Series(np.arange(10), index=[f'R{i}' for i in range(10)])\n",
    "print(\"Series truncate example (R2..R5):\\n\", s.truncate('R2','R5'), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 4) ITERATING OVER ROWS & COLUMNS\n",
    "# ============================\n",
    "# Recommended: avoid Python-level iteration when possible (use vectorized ops).\n",
    "# But when necessary:\n",
    "#  - df.iterrows(): yields (index, Series) per row (makes a copy of row -> slower)\n",
    "#  - df.itertuples(): yields namedtuples per row (faster)\n",
    "#  - for columns: for col in df.columns: df[col]  (vectorized)\n",
    "print(\"=== 4) ITERATING ROWS & COLUMNS ===\\n\")\n",
    "\n",
    "print(\"Using iterrows() (not recommended for heavy loops):\")\n",
    "for idx, row in df.head(3).iterrows():\n",
    "    print(\"index:\", idx, \"Name:\", row['Name'], \"Salary:\", row['Salary'])\n",
    "print()\n",
    "\n",
    "print(\"Using itertuples() (faster):\")\n",
    "for row in df.head(3).itertuples(index=True, name='Emp'):\n",
    "    # access by attribute: row.Name row.Salary\n",
    "    print(row)\n",
    "print()\n",
    "\n",
    "print(\"Iterating columns (column names):\")\n",
    "for col in df.columns:\n",
    "    print(\"col:\", col, \"dtype:\", df[col].dtype)\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 5) SORT_VALUES (single & multi-column)\n",
    "# ============================\n",
    "# df.sort_values(by, axis=0, ascending=True, inplace=False, na_position='last')\n",
    "# - by: column label or list of labels\n",
    "# - ascending: bool or list for multi-column\n",
    "# - na_position: 'first' or 'last'\n",
    "print(\"=== 5) SORT_VALUES ===\\n\")\n",
    "\n",
    "# Sort by single column ascending\n",
    "print(\"Sort by Age ascending:\\n\", df.sort_values(by='Age')[['Name','Age']], \"\\n\")\n",
    "\n",
    "# Sort by Salary descending\n",
    "print(\"Sort by Salary descending:\\n\", df.sort_values(by='Salary', ascending=False)[['Name','Salary']], \"\\n\")\n",
    "\n",
    "# Multi-column sort: by Dept then Salary descending\n",
    "print(\"Sort by Dept ascending, Salary descending:\\n\",\n",
    "      df.sort_values(by=['Dept','Salary'], ascending=[True, False])[['Dept','Name','Salary']], \"\\n\")\n",
    "\n",
    "# Stable sort and na_position example\n",
    "df_with_na = df.copy()\n",
    "df_with_na.loc[2,'Salary'] = np.nan\n",
    "print(\"Sort with NaN salary (na_position='first'):\\n\", df_with_na.sort_values('Salary', na_position='first')[['Name','Salary']], \"\\n\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 6) ADDING ONE ROW (several safe methods)\n",
    "# ============================\n",
    "# Methods:\n",
    "#  - loc (if index label unused): df.loc[new_label] = values\n",
    "#  - pd.concat([df, pd.DataFrame([row_dict])], ignore_index=True) (recommended)\n",
    "#  - append() was deprecated; use concat\n",
    "print(\"=== 6) ADD ONE ROW ===\\n\")\n",
    "\n",
    "# Using concat (recommended; avoids SettingWithCopy/warnings)\n",
    "new_row = {'Dept':'Eng','Name':'Grace','Age':27,'Salary':70000,'Months':7,'Country':'India','Seniority':'Jr','Monthly_Est':70000/12.0}\n",
    "df_added = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True, sort=False)\n",
    "print(\"After pd.concat to add one row:\\n\", df_added.tail(3), \"\\n\")\n",
    "\n",
    "# Using loc with integer index (only if index label is managed carefully)\n",
    "df_loc = df.copy()\n",
    "next_idx = df_loc.index.max() + 1\n",
    "df_loc.loc[next_idx] = ['QA','Hank',31,55000,11,'India','Mid',55000/12.0]  # careful: matching column order\n",
    "print(\"After df.loc[...] = ... (added one row):\\n\", df_loc.tail(3), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 7) GROUPBY: basic grouping & aggregations\n",
    "# ============================\n",
    "# df.groupby(by, axis=0, level=None, as_index=True, sort=True)\n",
    "# - by: column label(s) or mapping, or list of labels\n",
    "# - as_index: when True (default) group labels become index in result\n",
    "# After groupby you can call: .sum(), .mean(), .agg(), .size(), .count(), .apply(), etc.\n",
    "print(\"=== 7) GROUPBY BASIC ===\\n\")\n",
    "\n",
    "g = df.groupby('Dept')   # Group by Dept column\n",
    "print(\"Group keys:\", list(g.groups.keys()))\n",
    "print(\"\\nGroup sizes (number of rows per Dept):\\n\", g.size(), \"\\n\")\n",
    "\n",
    "print(\"Group mean of numeric columns:\\n\", g.mean(numeric_only=True), \"\\n\")\n",
    "print(\"Group sum of Salary:\\n\", g['Salary'].sum(), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 8) GROUPING ROWS (different examples)\n",
    "# ============================\n",
    "print(\"=== 8) GROUPING ROWS EXAMPLES ===\\n\")\n",
    "\n",
    "# groupby multiple columns\n",
    "g2 = df.groupby(['Dept','Seniority'])\n",
    "print(\"Group sizes by Dept and Seniority:\\n\", g2.size(), \"\\n\")\n",
    "\n",
    "# iterate groups (group_name, group_df)\n",
    "for name, group in g:\n",
    "    print(\"Dept:\", name, \"-> rows:\\n\", group[['Name','Salary']].to_string(index=False))\n",
    "print()\n",
    "\n",
    "# groupby then access a specific group's DataFrame\n",
    "eng_group = g.get_group('Eng')\n",
    "print(\"Group 'Eng' rows:\\n\", eng_group, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 9) COMBINING MULTIPLE COLUMNS IN GROUPBY WITH DICTIONARY AGGREGATIONS\n",
    "# ============================\n",
    "# df.groupby('key').agg({'col1': ['mean','sum'], 'col2': 'max', 'col3': ['min','count']})\n",
    "# - Pass a dict mapping column_name -> single agg or list of aggs\n",
    "# - You can also pass named aggregations (pandas >= 0.25) like: df.groupby('Dept').agg(avg_sal=('Salary','mean'))\n",
    "print(\"=== 9) GROUPBY with dict aggregations (multi-agg per column) ===\\n\")\n",
    "\n",
    "agg_result = df.groupby('Dept').agg({\n",
    "    'Salary': ['mean','sum','max'],     # for Salary compute mean, sum, max\n",
    "    'Age': 'mean',                      # for Age just mean\n",
    "    'Months': ['sum','count']           # for Months compute sum and count\n",
    "})\n",
    "print(\"Agg result with MultiIndex columns:\\n\", agg_result, \"\\n\")\n",
    "\n",
    "# Named aggregations (flat columns, more readable)\n",
    "agg_named = df.groupby('Dept').agg(\n",
    "    avg_salary = ('Salary','mean'),\n",
    "    total_salary = ('Salary','sum'),\n",
    "    max_salary = ('Salary','max'),\n",
    "    headcount = ('Name','count'),\n",
    "    avg_months = ('Months','mean')\n",
    ")\n",
    "print(\"Named aggregation (flat columns):\\n\", agg_named, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 10) EXTRA: useful groupby helpers\n",
    "# ============================\n",
    "print(\"=== 10) Groupby helpers & tips ===\\n\")\n",
    "print(\"- g.size()     : number of rows per group\")\n",
    "print(\"- g.count()    : non-NA counts per column per group\")\n",
    "print(\"- g.describe() : descriptive stats per group\")\n",
    "print(\"- g.agg(...)   : flexible aggregation (strings, callables, lists, dict)\")\n",
    "print(\"- Use as_index=False in groupby to keep group keys as columns instead of index\\n\")\n",
    "\n",
    "# Example: groupby with as_index=False to get group keys as columns\n",
    "print(\"groupby with as_index=False result:\\n\", df.groupby('Dept', as_index=False)['Salary'].mean(), \"\\n\")\n",
    "\n",
    "\n",
    "print(\"=== DONE: rows/cols/groupby examples ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba41c09",
   "metadata": {},
   "source": [
    "## Merging | Joining | Concatination | Comparing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d3c1e",
   "metadata": {},
   "source": [
    "1. https://www.geeksforgeeks.org/python/python-pandas-merging-joining-and-concatenating/\n",
    "\n",
    "2. https://www.geeksforgeeks.org/python/python-pandas-series-str-cat-to-concatenate-string/\n",
    "\n",
    "3. https://www.geeksforgeeks.org/python/python-pandas-dataframe-append/\n",
    "\n",
    "4. https://www.geeksforgeeks.org/pandas/python-pandas-series-append/\n",
    "\n",
    "5. https://www.geeksforgeeks.org/python/python-pandas-index-append/\n",
    "\n",
    "6. https://www.geeksforgeeks.org/python/python-pandas-series-combine/\n",
    "\n",
    "7. https://www.geeksforgeeks.org/python/add-a-row-at-top-in-pandas-dataframe/\n",
    "\n",
    "8. https://www.geeksforgeeks.org/python/python-pandas-str-join-to-join-string-list-elements-with-passed-delimiter/\n",
    "\n",
    "9. https://www.geeksforgeeks.org/python/join-two-text-columns-into-a-single-column-in-pandas/\n",
    "\n",
    "10. https://www.geeksforgeeks.org/python/how-to-compare-two-dataframes-with-pandas-compare/\n",
    "\n",
    "11. https://www.geeksforgeeks.org/python/how-to-compare-the-elements-of-the-two-pandas-series/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe09e258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SETUP: sample DataFrames & Series ===\n",
      "\n",
      "left:\n",
      "   key   A   B\n",
      "0  K0  A0  B0\n",
      "1  K1  A1  B1\n",
      "2  K2  A2  B2\n",
      "3  K3  A3  B3 \n",
      "\n",
      "right:\n",
      "   key   C   D\n",
      "0  K0  C0  D0\n",
      "1  K1  C1  D1\n",
      "2  K2  C2  D2\n",
      "3  K4  C4  D4 \n",
      "\n",
      "=== 1) pd.merge examples ===\n",
      "\n",
      "Inner merge on 'key' (intersection rows):\n",
      "   key   A   B   C   D\n",
      "0  K0  A0  B0  C0  D0\n",
      "1  K1  A1  B1  C1  D1\n",
      "2  K2  A2  B2  C2  D2 \n",
      "\n",
      "Left merge (all left rows, matched right columns filled NaN):\n",
      "   key   A   B    C    D\n",
      "0  K0  A0  B0   C0   D0\n",
      "1  K1  A1  B1   C1   D1\n",
      "2  K2  A2  B2   C2   D2\n",
      "3  K3  A3  B3  NaN  NaN \n",
      "\n",
      "Outer merge (union of keys):\n",
      "   key    A    B    C    D\n",
      "0  K0   A0   B0   C0   D0\n",
      "1  K1   A1   B1   C1   D1\n",
      "2  K2   A2   B2   C2   D2\n",
      "3  K3   A3   B3  NaN  NaN\n",
      "4  K4  NaN  NaN   C4   D4 \n",
      "\n",
      "Merge with different key names (left_on, right_on):\n",
      "   lkey   A   B rkey   C   D\n",
      "0   K0  A0  B0   K0  C0  D0\n",
      "1   K1  A1  B1   K1  C1  D1\n",
      "2   K2  A2  B2   K2  C2  D2 \n",
      "\n",
      "=== 2) DataFrame.join (index-wise) ===\n",
      "\n",
      "Index-join (left_index.join(right_index)):\n",
      "   key   A   B    C    D\n",
      "0  K0  A0  B0   C0   D0\n",
      "1  K1  A1  B1   C1   D1\n",
      "2  K2  A2  B2   C2   D2\n",
      "3  K3  A3  B3  NaN  NaN \n",
      "\n",
      "=== 3) pd.concat examples ===\n",
      "\n",
      "Concat rows (axis=0) - stack DataFrames (columns union):\n",
      "   key    A    B    C    D\n",
      "0  K0   A0   B0  NaN  NaN\n",
      "1  K1   A1   B1  NaN  NaN\n",
      "2  K2   A2   B2  NaN  NaN\n",
      "3  K3   A3   B3  NaN  NaN\n",
      "4  K0  NaN  NaN   C0   D0\n",
      "5  K1  NaN  NaN   C1   D1\n",
      "6  K2  NaN  NaN   C2   D2\n",
      "7  K4  NaN  NaN   C4   D4 \n",
      "\n",
      "Concat columns (axis=1) - side-by-side by index:\n",
      "   key    A    B    C    D\n",
      "0  K0   A0   B0   C0   D0\n",
      "1  K1   A1   B1   C1   D1\n",
      "2  K2   A2   B2   C2   D2\n",
      "3  K3   A3   B3  NaN  NaN\n",
      "4  K4  NaN  NaN   C4   D4 \n",
      "\n",
      "=== 4) String concatenation: Series.str.cat & str.join ===\n",
      "\n",
      "First:\n",
      " 0    John\n",
      "1    Jane\n",
      "2    Mary\n",
      "dtype: object\n",
      "Last:\n",
      " 0     Doe\n",
      "1     Roe\n",
      "2    None\n",
      "dtype: object\n",
      "s_first.str.cat(s_last, sep=' ', na_rep='') -> ['John Doe', 'Jane Roe', 'Mary ']\n",
      "Series of lists joined within each element (str.join): ['a-b', 'x-y-z', ''] \n",
      "\n",
      "=== 5) append() deprecated - use pd.concat instead ===\n",
      "\n",
      "pd.concat([dfA, dfB], ignore_index=True):\n",
      "    x\n",
      "0  1\n",
      "1  2\n",
      "2  3\n",
      "3  4 \n",
      "\n",
      "Index union of ix1 and ix2 -> [1, 2, 3, 4, 5] \n",
      "\n",
      "=== 6) Series.combine examples ===\n",
      "\n",
      "sA:\n",
      " a    10.0\n",
      "b     NaN\n",
      "c    30.0\n",
      "dtype: float64\n",
      "sB:\n",
      " a    1\n",
      "b    2\n",
      "c    3\n",
      "dtype: int64\n",
      "sA.combine(sB, lambda x,y: max(x,y), fill_value=-inf):\n",
      " a    10.0\n",
      "b     NaN\n",
      "c    30.0\n",
      "dtype: float64 \n",
      "\n",
      "=== 7) Add a row at the TOP ===\n",
      "\n",
      "Added row at top via concat:\n",
      "     Name  Age\n",
      "0   Zara   20\n",
      "1  Alice   25\n",
      "2    Bob   30 \n",
      "\n",
      "=== 8) Join two text columns into one ===\n",
      "\n",
      "df_names with full name via str.cat:\n",
      "   first last full\n",
      "0     A    X  A X\n",
      "1     B    Y  B Y \n",
      "\n",
      "Alternative full (vectorized +):\n",
      "   first last full full2\n",
      "0     A    X  A X   A X\n",
      "1     B    Y  B Y   B Y \n",
      "\n",
      "=== 9) DataFrame.compare and Series.compare ===\n",
      "\n",
      "df1:\n",
      "    A  B\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6 \n",
      "\n",
      "df2:\n",
      "     A   B\n",
      "0   1   4\n",
      "1  20  50\n",
      "2   3   6 \n",
      "\n",
      "df1.compare(df2) -> differences only:\n",
      "      A          B      \n",
      "  self other self other\n",
      "1  2.0  20.0  5.0  50.0 \n",
      "\n",
      "df1.compare(df2, keep_shape=True, keep_equal=True):\n",
      "      A          B      \n",
      "  self other self other\n",
      "0    1     1    4     4\n",
      "1    2    20    5    50\n",
      "2    3     3    6     6 \n",
      "\n",
      "s1.compare(s2):\n",
      "    self  other\n",
      "1   2.0   20.0 \n",
      "\n",
      "Elementwise equality (df1 == df2):\n",
      "        A      B\n",
      "0   True   True\n",
      "1  False  False\n",
      "2   True   True \n",
      "\n",
      "Any differences per row: (df1 != df2).any(axis=1):\n",
      " 0    False\n",
      "1     True\n",
      "2    False\n",
      "dtype: bool \n",
      "\n",
      "=== 10) Compare two Series in flexible ways ===\n",
      "\n",
      "sa:\n",
      " a    10.0\n",
      "b    20.0\n",
      "c     NaN\n",
      "d    40.0\n",
      "dtype: float64\n",
      "sb:\n",
      " a    10\n",
      "b    21\n",
      "c    30\n",
      "d    41\n",
      "dtype: int64 \n",
      "\n",
      "Difference sa - sb:\n",
      " a    0.0\n",
      "b   -1.0\n",
      "c    NaN\n",
      "d   -1.0\n",
      "dtype: float64 \n",
      "\n",
      "sa == sb ->\n",
      " a     True\n",
      "b    False\n",
      "c    False\n",
      "d    False\n",
      "dtype: bool \n",
      "\n",
      "sa.combine(sb, compare_vals) ->\n",
      " a          10.0\n",
      "b    (20.0, 21)\n",
      "c     (nan, 30)\n",
      "d    (40.0, 41)\n",
      "dtype: object \n",
      "\n",
      "=== ALL EXAMPLES COMPLETE ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas_merge_concat_compare.py\n",
    "# Demonstrates DataFrame/DataSeries merging/joining/concatenation, string concatenation,\n",
    "# append alternatives, index append note, Series.combine, adding a row at top,\n",
    "# and comparison functions (DataFrame.compare, Series.compare, elementwise).\n",
    "#\n",
    "# All examples are top-level and include comments explaining what each function does\n",
    "# and its important parameters.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n=== SETUP: sample DataFrames & Series ===\\n\")\n",
    "\n",
    "left = pd.DataFrame({\n",
    "    'key': ['K0','K1','K2','K3'],\n",
    "    'A'  : ['A0','A1','A2','A3'],\n",
    "    'B'  : ['B0','B1','B2','B3']\n",
    "})\n",
    "\n",
    "right = pd.DataFrame({\n",
    "    'key': ['K0','K1','K2','K4'],\n",
    "    'C'  : ['C0','C1','C2','C4'],\n",
    "    'D'  : ['D0','D1','D2','D4']\n",
    "})\n",
    "\n",
    "print(\"left:\\n\", left, \"\\n\")\n",
    "print(\"right:\\n\", right, \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 1) MERGE / JOIN (database-style joins)\n",
    "# ============================================================\n",
    "# pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, suffixes=('_x','_y'))\n",
    "# - how: 'left','right','outer','inner' (inner = intersection)\n",
    "# - on: column name(s) to join on when both frames share same name\n",
    "# - left_on/right_on: use when join keys have different names\n",
    "# - suffixes: suffixes for overlapping column names\n",
    "print(\"=== 1) pd.merge examples ===\\n\")\n",
    "\n",
    "# Inner join on 'key' (default how='inner')\n",
    "m_inner = pd.merge(left, right, on='key', how='inner', suffixes=('_L','_R'))\n",
    "print(\"Inner merge on 'key' (intersection rows):\\n\", m_inner, \"\\n\")\n",
    "\n",
    "# Left join (keep all rows from left)\n",
    "m_left = pd.merge(left, right, on='key', how='left')\n",
    "print(\"Left merge (all left rows, matched right columns filled NaN):\\n\", m_left, \"\\n\")\n",
    "\n",
    "# Outer join (union of keys)\n",
    "m_outer = pd.merge(left, right, on='key', how='outer', sort=True)\n",
    "print(\"Outer merge (union of keys):\\n\", m_outer, \"\\n\")\n",
    "\n",
    "# Merge on different key names example\n",
    "L2 = left.rename(columns={'key':'lkey'})\n",
    "R2 = right.rename(columns={'key':'rkey'})\n",
    "m_diff = pd.merge(L2, R2, left_on='lkey', right_on='rkey', how='inner')\n",
    "print(\"Merge with different key names (left_on, right_on):\\n\", m_diff, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) JOIN method on DataFrame (join on index)\n",
    "# ============================================================\n",
    "# df.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)\n",
    "# - joins columns of other to df by index (or on=column to use column on left as key)\n",
    "print(\"=== 2) DataFrame.join (index-wise) ===\\n\")\n",
    "\n",
    "left_idx = left.set_index('key')\n",
    "right_idx = right.set_index('key')\n",
    "\n",
    "# left_idx.join(right_idx, how='left') is like SQL left join on index\n",
    "joined = left_idx.join(right_idx, how='left', lsuffix='_L', rsuffix='_R')\n",
    "print(\"Index-join (left_index.join(right_index)):\\n\", joined.reset_index(), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) CONCATENATION (pd.concat)\n",
    "# ============================================================\n",
    "# pd.concat(objs, axis=0, join='outer', ignore_index=False, keys=None)\n",
    "# - axis=0 -> stack rows (one below another), axis=1 -> side-by-side (columns)\n",
    "# - join: 'outer' union of columns, 'inner' intersection\n",
    "# - ignore_index=True -> create new integer index\n",
    "print(\"=== 3) pd.concat examples ===\\n\")\n",
    "\n",
    "# Row-wise concatenation (stack rows) - axis=0\n",
    "cat_rows = pd.concat([left, right.rename(columns={'key':'key'})], axis=0, sort=False, ignore_index=True)\n",
    "print(\"Concat rows (axis=0) - stack DataFrames (columns union):\\n\", cat_rows, \"\\n\")\n",
    "\n",
    "# Column-wise concatenation (axis=1) - requires aligned index (or use ignore_index)\n",
    "cat_cols = pd.concat([left.set_index('key'), right.set_index('key')], axis=1, join='outer')\n",
    "print(\"Concat columns (axis=1) - side-by-side by index:\\n\", cat_cols.reset_index(), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Series.str.cat and str.join (string concatenation)\n",
    "# ============================================================\n",
    "# Series.str.cat(others=None, sep=None, na_rep=None) -> concatenate strings in Series with others\n",
    "# - others: Series, list-like of Series, or list-like of strings to concatenate elementwise\n",
    "# - sep: separator between parts\n",
    "# - na_rep: string representation for NA values (if you want to include them)\n",
    "# str.join is Python's join; pandas also supports Series.str.join on lists stored in Series.\n",
    "print(\"=== 4) String concatenation: Series.str.cat & str.join ===\\n\")\n",
    "\n",
    "s_first = pd.Series(['John','Jane','Mary'])\n",
    "s_last  = pd.Series(['Doe','Roe', None])\n",
    "\n",
    "print(\"First:\\n\", s_first)\n",
    "print(\"Last:\\n\", s_last)\n",
    "\n",
    "# Concatenate first + ' ' + last; fill missing last name with empty string using na_rep\n",
    "full = s_first.str.cat(s_last, sep=' ', na_rep='')\n",
    "print(\"s_first.str.cat(s_last, sep=' ', na_rep='') ->\", full.tolist())\n",
    "\n",
    "# If a Series contains lists of strings, use str.join to join items within each row\n",
    "s_lists = pd.Series([['a','b'], ['x','y','z'], []])\n",
    "joined_within = s_lists.str.join('-')   # joins list elements with '-'\n",
    "print(\"Series of lists joined within each element (str.join):\", joined_within.tolist(), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) append() deprecation & alternatives (DataFrame.append/Series.append/Index.append)\n",
    "# ============================================================\n",
    "# DataFrame.append and Series.append are deprecated (and in newer pandas removed).\n",
    "# Use pd.concat([...], ignore_index=...) instead.\n",
    "print(\"=== 5) append() deprecated - use pd.concat instead ===\\n\")\n",
    "\n",
    "dfA = pd.DataFrame({'x':[1,2]})\n",
    "dfB = pd.DataFrame({'x':[3,4]})\n",
    "\n",
    "# old: dfA.append(dfB, ignore_index=True)  # deprecated\n",
    "df_concat = pd.concat([dfA, dfB], ignore_index=True)  # recommended\n",
    "print(\"pd.concat([dfA, dfB], ignore_index=True):\\n\", df_concat, \"\\n\")\n",
    "\n",
    "# Index.append is also deprecated. Use Index.union or pd.concat for index objects.\n",
    "ix1 = pd.Index([1,2,3])\n",
    "ix2 = pd.Index([4,5])\n",
    "# old: ix1.append(ix2)  # deprecated\n",
    "ix_union = ix1.union(ix2)   # union keeps sorted unique values by default\n",
    "print(\"Index union of ix1 and ix2 ->\", ix_union.tolist(), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Series.combine (elementwise combine two series via a function)\n",
    "# ============================================================\n",
    "# Series.combine(other, func, fill_value=None)\n",
    "# - For each index label, if both present, apply func(s1_val, s2_val)\n",
    "# - If one is missing, use fill_value (if provided) for missing side\n",
    "# - returns a new Series aligned by union of indices\n",
    "print(\"=== 6) Series.combine examples ===\\n\")\n",
    "\n",
    "sA = pd.Series([10, np.nan, 30], index=['a','b','c'])\n",
    "sB = pd.Series([1, 2, 3], index=['a','b','c'])\n",
    "print(\"sA:\\n\", sA)\n",
    "print(\"sB:\\n\", sB)\n",
    "\n",
    "# combine by custom function: choose max of two (with NaN handled by fill_value)\n",
    "combined = sA.combine(sB, lambda x, y: max(x, y), fill_value=-np.inf)\n",
    "print(\"sA.combine(sB, lambda x,y: max(x,y), fill_value=-inf):\\n\", combined, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Add a row at the TOP of a DataFrame\n",
    "# ============================================================\n",
    "# Recommended: build a single-row DataFrame and pd.concat([new_row_df, df], ignore_index=...)\n",
    "# Careful with column order and dtypes.\n",
    "print(\"=== 7) Add a row at the TOP ===\\n\")\n",
    "\n",
    "df_people = pd.DataFrame({'Name':['Alice','Bob'], 'Age':[25,30]})\n",
    "new_top = pd.DataFrame([{'Name':'Zara','Age':20}])\n",
    "df_with_top = pd.concat([new_top, df_people], ignore_index=True)  # new top row now index 0\n",
    "print(\"Added row at top via concat:\\n\", df_with_top, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) str.join to join list elements or join columns into one string\n",
    "# ============================================================\n",
    "# To combine two text columns into single column use Series.str.cat or vectorized +\n",
    "print(\"=== 8) Join two text columns into one ===\\n\")\n",
    "\n",
    "df_names = pd.DataFrame({'first':['A','B'], 'last':['X','Y']})\n",
    "# Using str.cat with separator\n",
    "df_names['full'] = df_names['first'].str.cat(df_names['last'], sep=' ')\n",
    "print(\"df_names with full name via str.cat:\\n\", df_names, \"\\n\")\n",
    "\n",
    "# Alternative: using vectorized string addition (handles NaN differently)\n",
    "df_names['full2'] = df_names['first'] + ' ' + df_names['last']\n",
    "print(\"Alternative full (vectorized +):\\n\", df_names, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) Compare DataFrames and Series\n",
    "# ============================================================\n",
    "# DataFrame.compare(other, align_axis=1, keep_shape=False, keep_equal=False)\n",
    "# - returns the elementwise differences between two DataFrames in a compact form\n",
    "# - keep_shape=True retains original shape filling unequal places with NaN\n",
    "# - keep_equal=True shows also equal values (useful for debugging)\n",
    "print(\"=== 9) DataFrame.compare and Series.compare ===\\n\")\n",
    "\n",
    "df1 = pd.DataFrame({'A':[1,2,3], 'B':[4,5,6]})\n",
    "df2 = pd.DataFrame({'A':[1,20,3], 'B':[4,50,6]})\n",
    "\n",
    "print(\"df1:\\n\", df1, \"\\n\")\n",
    "print(\"df2:\\n\", df2, \"\\n\")\n",
    "\n",
    "# Compare to see elementwise differences (only differing elements shown by default)\n",
    "cmp = df1.compare(df2)    # columns are MultiIndex: (col, 'self') and (col, 'other')\n",
    "print(\"df1.compare(df2) -> differences only:\\n\", cmp, \"\\n\")\n",
    "\n",
    "# If you want the full shape with equal values retained:\n",
    "cmp_full = df1.compare(df2, keep_shape=True, keep_equal=True)\n",
    "print(\"df1.compare(df2, keep_shape=True, keep_equal=True):\\n\", cmp_full, \"\\n\")\n",
    "\n",
    "# Series.compare (pandas >= 1.1) works similarly for elementwise differences\n",
    "s1 = pd.Series([1,2,3])\n",
    "s2 = pd.Series([1,20,3])\n",
    "print(\"s1.compare(s2):\\n\", s1.compare(s2), \"\\n\")\n",
    "\n",
    "# Other comparison helpers:\n",
    "print(\"Elementwise equality (df1 == df2):\\n\", (df1 == df2), \"\\n\")\n",
    "print(\"Any differences per row: (df1 != df2).any(axis=1):\\n\", (df1 != df2).any(axis=1), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10) Compare two Series: equality, elementwise diff, and combine\n",
    "# ============================================================\n",
    "# To compare and produce a custom output use combine or map\n",
    "print(\"=== 10) Compare two Series in flexible ways ===\\n\")\n",
    "\n",
    "sa = pd.Series([10, 20, np.nan, 40], index=['a','b','c','d'])\n",
    "sb = pd.Series([10, 21, 30, 41], index=['a','b','c','d'])\n",
    "print(\"sa:\\n\", sa)\n",
    "print(\"sb:\\n\", sb, \"\\n\")\n",
    "\n",
    "# Elementwise difference (NaNs propagate)\n",
    "print(\"Difference sa - sb:\\n\", sa - sb, \"\\n\")\n",
    "\n",
    "# Elementwise boolean equality (treat NaNs as not equal)\n",
    "print(\"sa == sb ->\\n\", sa == sb, \"\\n\")\n",
    "\n",
    "# Using combine to choose which value to keep (example: prefer sa if equal else show tuple)\n",
    "def compare_vals(x, y):\n",
    "    if pd.isna(x) and pd.isna(y):\n",
    "        return np.nan\n",
    "    if x == y:\n",
    "        return x\n",
    "    return (x, y)\n",
    "\n",
    "comp_series = sa.combine(sb, compare_vals)\n",
    "print(\"sa.combine(sb, compare_vals) ->\\n\", comp_series, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DONE\n",
    "# ============================================================\n",
    "print(\"=== ALL EXAMPLES COMPLETE ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2211ecd",
   "metadata": {},
   "source": [
    "## Date Time and Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2999b",
   "metadata": {},
   "source": [
    "1. https://www.geeksforgeeks.org/python/python-working-with-date-and-time-using-pandas/\n",
    "\n",
    "2. https://www.geeksforgeeks.org/python/python-pandas-timestamp-timestamp/\n",
    "\n",
    "3. https://www.geeksforgeeks.org/python/python-pandas-timestamp-now/\n",
    "\n",
    "4. https://www.geeksforgeeks.org/python/python-pandas-timestamp-isoformat/\n",
    "\n",
    "5. https://www.geeksforgeeks.org/python/python-pandas-timestamp-date/\n",
    "\n",
    "6. https://www.geeksforgeeks.org/python/python-pandas-timestamp-replace/\n",
    "\n",
    "7. https://www.geeksforgeeks.org/python/python-pandas-to_datetime/\n",
    "\n",
    "8. https://www.geeksforgeeks.org/python/python-pandas-working-with-text-data/\n",
    "\n",
    "9. https://www.geeksforgeeks.org/python/python-pandas-series-str-lower-upper-and-title/\n",
    "\n",
    "10. https://www.geeksforgeeks.org/python/python-pandas-series-str-replace-to-replace-text-in-a-series/\n",
    "\n",
    "11. https://www.geeksforgeeks.org/pandas/python-pandas-series-replace/\n",
    "\n",
    "12. https://www.geeksforgeeks.org/python/python-pandas-series-str-strip-lstrip-and-rstrip/\n",
    "\n",
    "13. https://www.geeksforgeeks.org/python/python-pandas-tseries-offsets-dateoffset/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77183601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PANDAS DATETIME & TEXT (examples + explanations) ===\n",
      "\n",
      "1) pandas.Timestamp examples\n",
      "\n",
      "ts1: 2023-11-18 14:30:00   dtype: <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "ts2: 2023-11-18 14:30:00\n",
      "\n",
      "Timestamp.now() (local): 2025-11-18 02:08:18.299877\n",
      "now_local.isoformat(): 2025-11-18T02:08:18.299877\n",
      "now_local.date(): 2025-11-18  type: <class 'datetime.date'>\n",
      "now_local.timestamp() (float seconds since epoch): 1763431698.299877\n",
      "ts1.replace(year=2025, hour=8) -> 2025-11-18 08:30:00\n",
      "\n",
      "2) pd.to_datetime examples & parsing options\n",
      "\n",
      "Sample string Series:\n",
      " ['2021-01-02', '02-01-2021', '2021/03/04 12:30', 'April 5 2021', '20210506', 'not a date']\n",
      "\n",
      "Parsed with default (errors='coerce'):\n",
      " 0   2021-01-02\n",
      "1          NaT\n",
      "2          NaT\n",
      "3          NaT\n",
      "4          NaT\n",
      "5          NaT\n",
      "dtype: datetime64[ns]\n",
      "\n",
      "Parsed single string with format='%Y/%m/%d %H:%M': 2021-03-04 12:30:00\n",
      "\n",
      "Parsed with dayfirst=True:\n",
      " 0   2021-02-01\n",
      "1          NaT\n",
      "2          NaT\n",
      "3          NaT\n",
      "4          NaT\n",
      "5          NaT\n",
      "dtype: datetime64[ns]\n",
      "\n",
      "Parsed with errors='ignore' -> invalid remains as original string:\n",
      " 0          2021-01-02\n",
      "1          02-01-2021\n",
      "2    2021/03/04 12:30\n",
      "3        April 5 2021\n",
      "4            20210506\n",
      "5          not a date\n",
      "dtype: object\n",
      "\n",
      "DataFrame with parsed date column:\n",
      "    id    date_str       date\n",
      "0   1  01-02-2020 2020-02-01\n",
      "1   2  03-04-2021 2021-04-03\n",
      "2   3  2020/12/31        NaT\n",
      "\n",
      "3) Timezone examples (brief)\n",
      "\n",
      "UTC now: 2025-11-17 20:38:18.305216+00:00\n",
      "UTC -> Asia/Kolkata: 2025-11-18 02:08:18.305216+05:30\n",
      "\n",
      "4) DateOffset and common offsets (add/subtract)\n",
      "\n",
      "base: 2021-01-15 00:00:00\n",
      "base + DateOffset(months=1): 2021-02-15 00:00:00\n",
      "base + BusinessDay(1): 2021-01-18 00:00:00\n",
      "base + MonthEnd(0) -> month end of base's month: 2021-01-31 00:00:00\n",
      "base + MonthEnd(1) -> next month end: 2021-01-31 00:00:00\n",
      "\n",
      "5) Series.dt accessor examples\n",
      "\n",
      "s_time:\n",
      " 0   2021-01-02\n",
      "1   2022-05-06\n",
      "2   2020-12-31\n",
      "dtype: datetime64[ns]\n",
      "Year: [2021, 2022, 2020]\n",
      "Month: [1, 5, 12]\n",
      "Day: [2, 6, 31]\n",
      "Day of week (0=Mon): [5, 4, 3]\n",
      "Is month end?: [False, False, True]\n",
      "Day name: ['Saturday', 'Friday', 'Thursday']\n",
      "\n",
      "6) Series.str text operations (lower/upper/title/replace/strip) \n",
      "\n",
      "Original text series:\n",
      " ['  Hello World  ', 'Pandas is Great', 'foo,bar,baz', None, '  mixed CASE ']\n",
      "\n",
      "str.lower -> ['  hello world  ', 'pandas is great', 'foo,bar,baz', None, '  mixed case ']\n",
      "str.upper -> ['  HELLO WORLD  ', 'PANDAS IS GREAT', 'FOO,BAR,BAZ', None, '  MIXED CASE ']\n",
      "str.title -> ['  Hello World  ', 'Pandas Is Great', 'Foo,Bar,Baz', None, '  Mixed Case ']\n",
      "\n",
      "str.strip -> ['Hello World', 'Pandas is Great', 'foo,bar,baz', None, 'mixed CASE']\n",
      "str.lstrip -> ['Hello World  ', 'Pandas is Great', 'foo,bar,baz', None, 'mixed CASE ']\n",
      "str.rstrip -> ['  Hello World', 'Pandas is Great', 'foo,bar,baz', None, '  mixed CASE']\n",
      "\n",
      "str.replace(',','|') -> ['  Hello World  ', 'Pandas is Great', 'foo|bar|baz', None, '  mixed CASE ']\n",
      "Regex replace ' +': single space -> [' Hello World ', 'Pandas is Great', 'foo,bar,baz', None, ' mixed CASE ']\n",
      "Note: operations ignore/return NaN for None / non-string entries.\n",
      "\n",
      "7) Series.replace (value-based) vs Series.str.replace (string-based)\n",
      "\n",
      "s_vals: [1.0, 2.0, 3.0, nan, 2.0]\n",
      "s_vals.replace(2, 200) -> [1.0, 200.0, 3.0, nan, 200.0]\n",
      "s_vals.replace({1:10, 2:20}) -> [10.0, 20.0, 3.0, nan, 20.0]\n",
      "s_names.str.replace(r'^Mr\\. |^Ms\\. |^Dr\\. ', '', regex=True) -> ['John', 'Alice', 'Bob', None]\n",
      "\n",
      "8) Parsing formats & errors in pd.to_datetime\n",
      "\n",
      "Parsed with explicit format (fast): 0   2020-12-31\n",
      "1   2020-12-31\n",
      "2   2020-12-31\n",
      "3   2020-12-31\n",
      "4   2020-12-31\n",
      "dtype: datetime64[ns]\n",
      "pd.to_datetime(mixed, errors='coerce') -> 0   2020-01-01\n",
      "1          NaT\n",
      "2   2021-05-05\n",
      "dtype: datetime64[ns]\n",
      "\n",
      "9) Small worked example: cleaning & offset usage\n",
      "\n",
      "Raw:\n",
      "    order_id  order_date amount_str\n",
      "0       101  01/02/2021   1,200.50\n",
      "1       102  2021-03-15       $300\n",
      "2       103  15-04-2021        450 \n",
      "\n",
      "After cleaning amounts:\n",
      "   amount_str amount_clean  amount\n",
      "0   1,200.50      1200.50  1200.5\n",
      "1       $300          300   300.0\n",
      "2        450          450   450.0 \n",
      "\n",
      "Parsed order_dt:\n",
      "    order_date   order_dt\n",
      "0  01/02/2021 2021-02-01\n",
      "1  2021-03-15        NaT\n",
      "2  15-04-2021        NaT \n",
      "\n",
      "Add month_end column using MonthEnd(0):\n",
      "     order_dt  month_end\n",
      "0 2021-02-01 2021-02-28\n",
      "1        NaT        NaT\n",
      "2        NaT        NaT \n",
      "\n",
      "\n",
      "10) Notes & gotchas\n",
      "\n",
      "- pd.to_datetime is flexible but can be slow on huge Series; pass format= when possible.\n",
      "- Use errors='coerce' when you want bad parses to become NaT instead of crashing.\n",
      "- Use Series.str.* for vectorized string ops (they ignore NaN and return NaN for non-strings).\n",
      "- Use Series.replace for value-based replacement (numbers, NaNs, mapping), and Series.str.replace for substring/regex replacement.\n",
      "- Offsets (MonthEnd, BusinessDay, DateOffset) are powerful for calendar-aware arithmetic.\n",
      "- For time zone handling, use tz-aware parsing and tz_convert/tz_localize as needed.\n",
      "\n",
      "=== DONE: pandas datetime & text examples ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_18704\\4052672199.py:86: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  parsed_ignore = pd.to_datetime(samples, errors='ignore')\n"
     ]
    }
   ],
   "source": [
    "# pandas_datetime_text_examples.py\n",
    "# Demonstrates: pandas Timestamp creation & methods, pd.to_datetime parsing options,\n",
    "# Timestamp.now(), isoformat(), date(), replace(), DateOffset and common offsets,\n",
    "# Series.str methods (lower, upper, title, replace, strip), Series.replace (non-string replacement),\n",
    "# and examples of parsing with formats, dayfirst, errors handling.\n",
    "#\n",
    "# Inline comments explain what each function does and describe important parameters.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries import offsets\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"\\n=== PANDAS DATETIME & TEXT (examples + explanations) ===\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) pandas.Timestamp - a pandas scalar for datetimes\n",
    "# ============================================================\n",
    "# pd.Timestamp is pandas' equivalent of Python's datetime but with more functionality (vectorized aware).\n",
    "# Useful methods: .now(), .isoformat(), .date(), .replace(), .timestamp()\n",
    "print(\"1) pandas.Timestamp examples\\n\")\n",
    "\n",
    "# Create Timestamp from string or datetime\n",
    "ts1 = pd.Timestamp(\"2023-11-18 14:30:00\")    # parse ISO-like string\n",
    "ts2 = pd.Timestamp(datetime(2023, 11, 18, 14, 30))  # from python datetime\n",
    "\n",
    "print(\"ts1:\", ts1, \"  dtype:\", type(ts1))\n",
    "print(\"ts2:\", ts2)\n",
    "\n",
    "# Timestamp.now(tz=None) -> current local time or with timezone tz (tz can be tzinfo or string)\n",
    "# Example: now in local timezone (no tz argument)\n",
    "now_local = pd.Timestamp.now()\n",
    "print(\"\\nTimestamp.now() (local):\", now_local)\n",
    "\n",
    "# .isoformat() -> ISO 8601 string representation (same as datetime.isoformat)\n",
    "print(\"now_local.isoformat():\", now_local.isoformat())\n",
    "\n",
    "# .date() -> returns a python date object (year, month, day only)\n",
    "print(\"now_local.date():\", now_local.date(), \" type:\", type(now_local.date()))\n",
    "\n",
    "# .timestamp() -> POSIX timestamp (seconds since epoch as float)\n",
    "print(\"now_local.timestamp() (float seconds since epoch):\", now_local.timestamp())\n",
    "\n",
    "# .replace() -> similar to datetime.replace, returns new Timestamp with replaced components\n",
    "# Signature: ts.replace(year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, tzinfo=None)\n",
    "ts_replaced = ts1.replace(year=2025, hour=8)   # change year and hour\n",
    "print(\"ts1.replace(year=2025, hour=8) ->\", ts_replaced)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) pd.to_datetime - parse strings to datetimes (very important)\n",
    "# ============================================================\n",
    "# pd.to_datetime(arg, errors='raise', format=None, dayfirst=False, yearfirst=False, utc=None)\n",
    "# - arg: scalar, list-like, Series, DataFrame (columns)\n",
    "# - errors: 'raise' (default) -> raise on parse error, 'coerce' -> set parse-fail to NaT, 'ignore' -> return original\n",
    "# - format: provide a format string (faster & strict) e.g. \"%d-%m-%Y %H:%M:%S\"\n",
    "# - dayfirst/yearfirst: interpret ambiguous dates like 01/02/2020\n",
    "# - utc: if True, return timezone-aware UTC timestamps\n",
    "print(\"\\n2) pd.to_datetime examples & parsing options\\n\")\n",
    "\n",
    "samples = pd.Series([\n",
    "    \"2021-01-02\",\n",
    "    \"02-01-2021\",        # ambiguous format\n",
    "    \"2021/03/04 12:30\",\n",
    "    \"April 5 2021\",\n",
    "    \"20210506\",          # compact format YYYYMMDD\n",
    "    \"not a date\"         # invalid\n",
    "])\n",
    "print(\"Sample string Series:\\n\", samples.tolist())\n",
    "\n",
    "# Default parsing (best-effort). Ambiguous forms are guessed.\n",
    "parsed_default = pd.to_datetime(samples, errors='coerce')  # invalid -> NaT\n",
    "print(\"\\nParsed with default (errors='coerce'):\\n\", parsed_default)\n",
    "\n",
    "# If you know the format, pass `format` for speed & correctness:\n",
    "# Example: format for \"2021/03/04 12:30\" is \"%Y/%m/%d %H:%M\"\n",
    "known_fmt = pd.to_datetime(\"2021/03/04 12:30\", format=\"%Y/%m/%d %H:%M\")\n",
    "print(\"\\nParsed single string with format='%Y/%m/%d %H:%M':\", known_fmt)\n",
    "\n",
    "# dayfirst=True (treat '02-01-2021' as 2 Jan 2021)\n",
    "parsed_dayfirst = pd.to_datetime(samples, dayfirst=True, errors='coerce')\n",
    "print(\"\\nParsed with dayfirst=True:\\n\", parsed_dayfirst)\n",
    "\n",
    "# errors='ignore' returns original input when parse fails\n",
    "parsed_ignore = pd.to_datetime(samples, errors='ignore')\n",
    "print(\"\\nParsed with errors='ignore' -> invalid remains as original string:\\n\", parsed_ignore)\n",
    "\n",
    "\n",
    "# Parse a DataFrame with a date column:\n",
    "df_dates = pd.DataFrame({\n",
    "    'id': [1,2,3],\n",
    "    'date_str': ['01-02-2020', '03-04-2021', '2020/12/31']\n",
    "})\n",
    "# Convert column to datetime in place:\n",
    "df_dates['date'] = pd.to_datetime(df_dates['date_str'], dayfirst=True, errors='coerce')\n",
    "print(\"\\nDataFrame with parsed date column:\\n\", df_dates)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Working with timezone-aware Timestamps (brief)\n",
    "# ============================================================\n",
    "print(\"\\n3) Timezone examples (brief)\\n\")\n",
    "\n",
    "# Create timezone-aware timestamp: pass tz argument or localize/convert\n",
    "utc_now = pd.Timestamp.now(tz='UTC')   # timezone specified by string\n",
    "print(\"UTC now:\", utc_now)\n",
    "\n",
    "# Convert between timezones\n",
    "india = utc_now.tz_convert('Asia/Kolkata')   # convert tz-aware ts to Asia/Kolkata\n",
    "print(\"UTC -> Asia/Kolkata:\", india)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) DateOffset & offsets: add months, business days, month ends etc.\n",
    "# ============================================================\n",
    "# pandas.tseries.offsets.DateOffset & prebuilt offsets (MonthEnd, MonthBegin, BusinessDay, BDay, YearEnd etc.)\n",
    "print(\"\\n4) DateOffset and common offsets (add/subtract)\\n\")\n",
    "\n",
    "base = pd.Timestamp(\"2021-01-15\")\n",
    "print(\"base:\", base)\n",
    "\n",
    "# Add 1 calendar month (DateOffset months=1)\n",
    "one_month_later = base + offsets.DateOffset(months=1)  # result: 2021-02-15\n",
    "print(\"base + DateOffset(months=1):\", one_month_later)\n",
    "\n",
    "# Add 1 business day (BusinessDay)\n",
    "bd_plus1 = base + offsets.BusinessDay(1)\n",
    "print(\"base + BusinessDay(1):\", bd_plus1)\n",
    "\n",
    "# MonthEnd offset: roll forward to month end\n",
    "me = base + offsets.MonthEnd(0)   # MonthEnd(0) moves to month end of the date if not already\n",
    "print(\"base + MonthEnd(0) -> month end of base's month:\", me)\n",
    "\n",
    "# Use MonthEnd(1) to move to next month end:\n",
    "print(\"base + MonthEnd(1) -> next month end:\", base + offsets.MonthEnd(1))\n",
    "\n",
    "# Custom DateOffset: e.g. 3 months and 10 days\n",
    "#custom = base + (offsets.DateOffset(months=3) + offsets.DateOffset(days=10))\n",
    "#print(\"custom offset 3 months + 10 days:\", custom)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Vectorized datetime accessors on Series: .dt (year, month, day, weekday, etc.)\n",
    "# ============================================================\n",
    "print(\"\\n5) Series.dt accessor examples\\n\")\n",
    "\n",
    "s_time = pd.Series(pd.to_datetime([\"2021-01-02\", \"2022-05-06\", \"2020-12-31\"]))\n",
    "print(\"s_time:\\n\", s_time)\n",
    "\n",
    "# Extract components\n",
    "print(\"Year:\", s_time.dt.year.tolist())\n",
    "print(\"Month:\", s_time.dt.month.tolist())\n",
    "print(\"Day:\", s_time.dt.day.tolist())\n",
    "print(\"Day of week (0=Mon):\", s_time.dt.dayofweek.tolist())\n",
    "print(\"Is month end?:\", s_time.dt.is_month_end.tolist())\n",
    "print(\"Day name:\", s_time.dt.day_name().tolist())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Text / string operations on Series (Series.str.*)\n",
    "# ============================================================\n",
    "print(\"\\n6) Series.str text operations (lower/upper/title/replace/strip) \\n\")\n",
    "\n",
    "s_text = pd.Series([\n",
    "    \"  Hello World  \",\n",
    "    \"Pandas is Great\",\n",
    "    \"foo,bar,baz\",\n",
    "    None,          # missing value example (NaN-like)\n",
    "    \"  mixed CASE \"\n",
    "])\n",
    "print(\"Original text series:\\n\", s_text.tolist())\n",
    "\n",
    "# str.lower() / str.upper() / str.title()\n",
    "# These are vectorized string operations that operate on each non-missing element.\n",
    "print(\"\\nstr.lower ->\", s_text.str.lower().tolist())\n",
    "print(\"str.upper ->\", s_text.str.upper().tolist())\n",
    "print(\"str.title ->\", s_text.str.title().tolist())\n",
    "\n",
    "# str.strip / lstrip / rstrip (remove whitespace or characters)\n",
    "# .str.strip(chars=None) -> remove leading/trailing whitespace or characters in 'chars'\n",
    "print(\"\\nstr.strip ->\", s_text.str.strip().tolist())     # removes leading/trailing spaces\n",
    "print(\"str.lstrip ->\", s_text.str.lstrip().tolist())     # remove left only\n",
    "print(\"str.rstrip ->\", s_text.str.rstrip().tolist())     # remove right only\n",
    "\n",
    "# str.replace(pattern, repl, n=-1, regex=True)\n",
    "# - pattern: substring or regex\n",
    "# - repl: replacement text\n",
    "# - regex: whether to treat pattern as regex (True by default)\n",
    "print(\"\\nstr.replace(',','|') ->\", s_text.str.replace(',', '|', regex=False).tolist())\n",
    "\n",
    "# If you need regex-based replacement, use regex=True (default)\n",
    "print(\"Regex replace ' +': single space ->\", s_text.str.replace(r'\\s+', ' ', regex=True).tolist())\n",
    "\n",
    "# If series contains lists or other objects, str.* operations return NaN for non-strings\n",
    "print(\"Note: operations ignore/return NaN for None / non-string entries.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Series.replace (non-string replacement) vs Series.str.replace\n",
    "# ============================================================\n",
    "print(\"\\n7) Series.replace (value-based) vs Series.str.replace (string-based)\\n\")\n",
    "\n",
    "s_vals = pd.Series([1, 2, 3, np.nan, 2])\n",
    "print(\"s_vals:\", s_vals.tolist())\n",
    "\n",
    "# Series.replace(to_replace, value, inplace=False)\n",
    "# - Used for replacing values (works across dtypes)\n",
    "# - to_replace can be scalar, list, dict, or regex (if regex=True)\n",
    "print(\"s_vals.replace(2, 200) ->\", s_vals.replace(2, 200).tolist())\n",
    "\n",
    "# Replace multiple values using dict mapping\n",
    "print(\"s_vals.replace({1:10, 2:20}) ->\", s_vals.replace({1:10, 2:20}).tolist())\n",
    "\n",
    "# For string patterns in text Series, use Series.str.replace (above).\n",
    "# Example:\n",
    "s_names = pd.Series(['Mr. John', 'Ms. Alice', 'Dr. Bob', None])\n",
    "print(\"s_names.str.replace(r'^Mr\\\\. |^Ms\\\\. |^Dr\\\\. ', '', regex=True) ->\",\n",
    "      s_names.str.replace(r'^(Mr\\. |Ms\\. |Dr\\. )', '', regex=True).tolist())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) Parsing with custom format, errors handling and performance tips\n",
    "# ============================================================\n",
    "print(\"\\n8) Parsing formats & errors in pd.to_datetime\\n\")\n",
    "\n",
    "# If the column is large and format is uniform, pass format to speed up and avoid ambiguity.\n",
    "# e.g., for \"31-12-2020\" use format=\"%d-%m-%Y\"\n",
    "large = pd.Series(['31-12-2020'] * 5)\n",
    "parsed_fast = pd.to_datetime(large, format='%d-%m-%Y', errors='raise')   # raises on mismatch\n",
    "print(\"Parsed with explicit format (fast):\", parsed_fast)\n",
    "\n",
    "# To safely coerce invalid entries to NaT:\n",
    "mixed = pd.Series(['2020-01-01', 'invalid', '2021-05-05'])\n",
    "print(\"pd.to_datetime(mixed, errors='coerce') ->\", pd.to_datetime(mixed, errors='coerce'))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) Small worked example: read CSV-like data with dates & text, clean, and compute month-end\n",
    "# ============================================================\n",
    "print(\"\\n9) Small worked example: cleaning & offset usage\\n\")\n",
    "\n",
    "# Sample data frame representing a CSV read result where dates and amount columns are strings\n",
    "df_raw = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103],\n",
    "    'order_date': ['01/02/2021', '2021-03-15', '15-04-2021'],   # mixed formats\n",
    "    'amount_str': ['1,200.50', '$300', '450']\n",
    "})\n",
    "print(\"Raw:\\n\", df_raw, \"\\n\")\n",
    "\n",
    "# Normalize amount_str -> numeric\n",
    "df_raw['amount_clean'] = (df_raw['amount_str']\n",
    "                           .astype(str)\n",
    "                           .str.replace(',', '', regex=False)              # remove thousands comma\n",
    "                           .str.replace(r'[^\\d\\.\\-]', '', regex=True))    # remove $ and other non-numeric chars\n",
    "df_raw['amount'] = pd.to_numeric(df_raw['amount_clean'], errors='coerce')\n",
    "print(\"After cleaning amounts:\\n\", df_raw[['amount_str','amount_clean','amount']], \"\\n\")\n",
    "\n",
    "# Parse order_date: use dayfirst heuristic (some are dd/mm, some are yyyy-mm-dd)\n",
    "df_raw['order_dt'] = pd.to_datetime(df_raw['order_date'], dayfirst=True, errors='coerce')\n",
    "print(\"Parsed order_dt:\\n\", df_raw[['order_date','order_dt']], \"\\n\")\n",
    "\n",
    "# Compute month end for each parsed date using MonthEnd offset\n",
    "df_raw['month_end'] = df_raw['order_dt'] + offsets.MonthEnd(0)   # nearest month end in same month\n",
    "print(\"Add month_end column using MonthEnd(0):\\n\", df_raw[['order_dt','month_end']], \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10) Common gotchas & notes (short)\n",
    "# ============================================================\n",
    "print(\"\\n10) Notes & gotchas\\n\")\n",
    "print(\"- pd.to_datetime is flexible but can be slow on huge Series; pass format= when possible.\")\n",
    "print(\"- Use errors='coerce' when you want bad parses to become NaT instead of crashing.\")\n",
    "print(\"- Use Series.str.* for vectorized string ops (they ignore NaN and return NaN for non-strings).\")\n",
    "print(\"- Use Series.replace for value-based replacement (numbers, NaNs, mapping), and Series.str.replace for substring/regex replacement.\")\n",
    "print(\"- Offsets (MonthEnd, BusinessDay, DateOffset) are powerful for calendar-aware arithmetic.\")\n",
    "print(\"- For time zone handling, use tz-aware parsing and tz_convert/tz_localize as needed.\\n\")\n",
    "\n",
    "print(\"=== DONE: pandas datetime & text examples ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992c297",
   "metadata": {},
   "source": [
    "## CSV and Excel Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33474620",
   "metadata": {},
   "source": [
    "1. https://www.geeksforgeeks.org/pandas/python-read-csv-using-pandas-read_csv/\n",
    "\n",
    "2. https://www.geeksforgeeks.org/pandas/saving-a-pandas-dataframe-as-a-csv/\n",
    "\n",
    "3. https://www.geeksforgeeks.org/python/creating-a-dataframe-using-excel-files/\n",
    "\n",
    "4. https://www.geeksforgeeks.org/python/python-working-with-pandas-and-xlsxwriter-set-1/\n",
    "\n",
    "5. https://www.geeksforgeeks.org/python/python-working-with-pandas-and-xlsxwriter-set-2/\n",
    "\n",
    "6. https://www.geeksforgeeks.org/python/python-working-with-pandas-and-xlsxwriter-set-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5677a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_io_csv_excel_examples.py\n",
    "# Demonstrates reading/writing CSV and Excel files with pandas.\n",
    "# - pd.read_csv: common parameters (sep, header, index_col, usecols, dtype, parse_dates, thousands)\n",
    "# - DataFrame.to_csv: index control, header, na_rep, float_format\n",
    "# - pd.read_excel: sheet_name, usecols, skiprows\n",
    "# - pd.ExcelWriter + engine='xlsxwriter': write multiple sheets, formatting, column width, charts, conditional formatting\n",
    "#\n",
    "# Run: python pandas_io_csv_excel_examples.py\n",
    "# Requires: pandas, openpyxl, XlsxWriter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "OUT = \"out_io_examples\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "print(\"\\n=== SAMPLE DATAFRAME (used for writing examples) ===\\n\")\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"OrderID\": [1001, 1002, 1003, 1004],\n",
    "    \"Customer\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n",
    "    \"Date\": pd.to_datetime([\"2021-01-02\", \"2021-01-05\", \"2021-01-07\", \"2021-01-10\"]),\n",
    "    \"Quantity\": [2, 1, 5, 3],\n",
    "    \"UnitPrice\": [12.50, 9.99, 4.75, 20.0]\n",
    "})\n",
    "df[\"Total\"] = df[\"Quantity\"] * df[\"UnitPrice\"]\n",
    "\n",
    "print(df)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 1) pd.read_csv: common usage & parameters\n",
    "# ============================================================\n",
    "#\n",
    "# pd.read_csv(filepath_or_buffer, sep=',', header='infer', index_col=None,\n",
    "#             usecols=None, dtype=None, parse_dates=False, dayfirst=False,\n",
    "#             thousands=None, na_values=None, skiprows=None, nrows=None)\n",
    "#\n",
    "# Important parameters:\n",
    "#  - sep: delimiter (',' default). Use '\\t' for TSV.\n",
    "#  - header: row number(s) to use as column names (0-based). None if no header.\n",
    "#  - index_col: column(s) to use as index.\n",
    "#  - usecols: subset of columns to read (list or callable).\n",
    "#  - dtype: dict to force types for columns.\n",
    "#  - parse_dates: column name(s) to parse as datetimes (True/list/dict).\n",
    "#  - thousands: character used as thousands separator (e.g., ',').\n",
    "#  - na_values: additional strings to recognize as NA.\n",
    "#  - skiprows, nrows: read subset of file.\n",
    "#\n",
    "# We'll create a CSV first to demonstrate reading options.\n",
    "\n",
    "csv_path = os.path.join(OUT, \"orders_sample.csv\")\n",
    "df.to_csv(csv_path, index=False)   # write CSV to disk (no index column)\n",
    "print(\"Wrote sample CSV to:\", csv_path)\n",
    "\n",
    "# Example A: simple read (auto-infer header)\n",
    "print(\"\\nread_csv basic (auto-infer):\")\n",
    "df_read = pd.read_csv(csv_path)\n",
    "print(df_read.dtypes)\n",
    "print(df_read.head(), \"\\n\")\n",
    "\n",
    "# Example B: read and parse Date as datetime (parse_dates)\n",
    "print(\"read_csv with parse_dates=['Date'] -> Date column becomes datetime dtype:\")\n",
    "df_read_dates = pd.read_csv(csv_path, parse_dates=[\"Date\"])\n",
    "print(df_read_dates.dtypes)\n",
    "print(df_read_dates.head(), \"\\n\")\n",
    "\n",
    "# Example C: read with dtype forcing and thousands handling\n",
    "# (use when numeric columns may contain thousands separators like '1,234')\n",
    "csv_with_thousands = os.path.join(OUT, \"orders_thousands.csv\")\n",
    "df_big = df.copy()\n",
    "df_big.loc[0, \"Total\"] = 1234567.89\n",
    "df_big.to_csv(csv_with_thousands, index=False, float_format=\"%.2f\")  # write large number\n",
    "# Manually add a thousands comma to demonstrate (not necessary here but shows how to parse)\n",
    "# Read forcing dtype (Total as float) and thousands=',' if numbers contain commas\n",
    "df_read_thousands = pd.read_csv(csv_with_thousands, dtype={\"OrderID\": int}, thousands=',')\n",
    "print(\"read_csv with thousands=',' and dtype={'OrderID':int} -> dtypes:\\n\", df_read_thousands.dtypes, \"\\n\")\n",
    "\n",
    "# Example D: read subset of columns via usecols and skiprows\n",
    "print(\"read_csv with usecols=['OrderID','Total'] and nrows=2:\")\n",
    "df_read_subset = pd.read_csv(csv_path, usecols=['OrderID','Total'], nrows=2)\n",
    "print(df_read_subset, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DataFrame.to_csv: options when writing CSV\n",
    "# ============================================================\n",
    "#\n",
    "# df.to_csv(path_or_buf, sep=',', na_rep='', float_format=None, columns=None,\n",
    "#           header=True, index=True, index_label=None)\n",
    "#\n",
    "# Useful params:\n",
    "#  - na_rep: string to represent missing values\n",
    "#  - float_format: format string for floats e.g. '%.2f'\n",
    "#  - columns: subset and order of columns to write\n",
    "#  - index: whether to write row index\n",
    "#  - index_label: label for index column when written\n",
    "#\n",
    "csv_out2 = os.path.join(OUT, \"orders_custom.csv\")\n",
    "df.to_csv(csv_out2, index=False, na_rep=\"NA\", float_format=\"%.2f\", columns=[\"OrderID\",\"Customer\",\"Total\"])\n",
    "print(\"Wrote CSV with selected columns and formatting to:\", csv_out2, \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) pd.read_excel: reading Excel files\n",
    "# ============================================================\n",
    "#\n",
    "# pd.read_excel(io, sheet_name=0, header=0, names=None, index_col=None,\n",
    "#               usecols=None, skiprows=None, nrows=None, dtype=None, parse_dates=False)\n",
    "#\n",
    "# Important parameters:\n",
    "#  - sheet_name: sheet name (str) or index (0-based) or list/None('all' to read all)\n",
    "#  - usecols: e.g., \"A:C\" or [0,2] or list of names\n",
    "#  - skiprows: rows to skip at top\n",
    "#  - nrows: limit to N rows\n",
    "#\n",
    "# To demonstrate, we'll create an Excel file first.\n",
    "\n",
    "excel_path = os.path.join(OUT, \"orders_sample.xlsx\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) Writing Excel with formatting via XlsxWriter engine\n",
    "# ============================================================\n",
    "#\n",
    "# Use pd.ExcelWriter(path, engine='xlsxwriter') as writer:\n",
    "#   df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "#   workbook = writer.book   # xlsxwriter Workbook\n",
    "#   worksheet = writer.sheets['Sheet1']  # xlsxwriter Worksheet\n",
    "# Then you can use xlsxwriter APIs to set formats, column widths, create charts, conditional formatting, etc.\n",
    "#\n",
    "print(\"Writing Excel with formatting using XlsxWriter...\")\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine='xlsxwriter', datetime_format='yyyy-mm-dd', date_format='yyyy-mm-dd') as writer:\n",
    "    # write the main dataframe to sheet 'Orders'\n",
    "    df.to_excel(writer, sheet_name='Orders', index=False, startrow=1, header=False)  # we'll write header manually with format\n",
    "\n",
    "    workbook  = writer.book\n",
    "    worksheet = writer.sheets['Orders']\n",
    "\n",
    "    # Create some formats\n",
    "    header_fmt = workbook.add_format({'bold': True, 'bg_color': '#DCE6F1', 'border':1})\n",
    "    money_fmt  = workbook.add_format({'num_format': '#,##0.00', 'align': 'right'})\n",
    "    date_fmt   = workbook.add_format({'num_format': 'yyyy-mm-dd'})\n",
    "    center_fmt = workbook.add_format({'align': 'center'})\n",
    "\n",
    "    # Write the header with formatting (we used header=False above)\n",
    "    for col_num, value in enumerate(df.columns.values):\n",
    "        worksheet.write(0, col_num, value, header_fmt)\n",
    "\n",
    "    # Set column widths & formats: (col index, first_col, last_col)\n",
    "    worksheet.set_column(0, 0, 10, center_fmt)    # OrderID width 10\n",
    "    worksheet.set_column(1, 1, 15)                # Customer width 15\n",
    "    worksheet.set_column(2, 2, 12, date_fmt)      # Date formatted as date\n",
    "    worksheet.set_column(3, 4, 12, money_fmt)     # Quantity and UnitPrice (we set Money format on UnitPrice & Total)\n",
    "\n",
    "    # Apply conditional formatting to Total column (last column index = len(df.columns)-1)\n",
    "    total_col = len(df.columns) - 1\n",
    "    # Format rows where Total > 50 with green fill\n",
    "    worksheet.conditional_format(1, total_col, len(df), total_col,\n",
    "                                 {'type': 'cell', 'criteria': '>', 'value': 50,\n",
    "                                  'format': workbook.add_format({'bg_color': '#C6EFCE'})})\n",
    "\n",
    "    # Create a simple chart (Total by OrderID)\n",
    "    chart = workbook.add_chart({'type': 'column'})\n",
    "    # chart.add_series requires Excel-like ranges: sheet!$C$2:$C$5 etc.\n",
    "    # We reference the sheet by name and the data range (note: row/col are zero-based to xlsxwriter)\n",
    "    chart.add_series({\n",
    "        'name': 'Total',\n",
    "        'categories': ['Orders', 1, 0, len(df), 0],   # OrderID column used as categories\n",
    "        'values':     ['Orders', 1, total_col, len(df), total_col],\n",
    "        'fill':       {'color': '#5B9BD5'}\n",
    "    })\n",
    "    chart.set_title({'name': 'Order Totals'})\n",
    "    chart.set_x_axis({'name': 'OrderID'})\n",
    "    chart.set_y_axis({'name': 'Total', 'num_format': '#,##0.00'})\n",
    "\n",
    "    # Insert the chart in the sheet\n",
    "    worksheet.insert_chart('G2', chart, {'x_scale': 1.2, 'y_scale': 1.2})\n",
    "\n",
    "    # Optionally write another sheet (summary)\n",
    "    summary = df.groupby('Customer', as_index=False)['Total'].sum()\n",
    "    summary.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    # you can format summary similarly\n",
    "print(\"Excel written to:\", excel_path)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) pd.read_excel usage examples (reading back Excel)\n",
    "# ============================================================\n",
    "print(\"Reading back Excel sheets with pd.read_excel:\")\n",
    "\n",
    "# Read a specific sheet by name\n",
    "df_orders = pd.read_excel(excel_path, sheet_name='Orders', skiprows=0, engine='openpyxl')\n",
    "print(\"Read 'Orders' sheet, columns:\", df_orders.columns.tolist())\n",
    "\n",
    "# Read all sheets (sheet_name=None returns dict of DataFrames)\n",
    "all_sheets = pd.read_excel(excel_path, sheet_name=None, engine='openpyxl')\n",
    "print(\"Read all sheets -> keys:\", list(all_sheets.keys()))\n",
    "\n",
    "# Read with usecols to only read specific columns (e.g., OrderID and Total)\n",
    "df_orders_small = pd.read_excel(excel_path, sheet_name='Orders', usecols=['OrderID','Total'], engine='openpyxl')\n",
    "print(\"Read only OrderID and Total columns:\\n\", df_orders_small.head(), \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Good practices & tips (short)\n",
    "# ============================================================\n",
    "print(\"Good practices & tips:\")\n",
    "print(\"- For large CSVs, pass 'usecols' and 'dtype' to reduce memory.\")\n",
    "print(\"- If CSV numbers have thousands separator, pass thousands=','; for custom NA strings use na_values.\")\n",
    "print(\"- When reading dates, pass parse_dates to get datetime dtype immediately (or do pd.to_datetime after read).\")\n",
    "print(\"- Use pd.ExcelWriter with engine='xlsxwriter' for rich Excel output (formats, charts, conditional formatting).\")\n",
    "print(\"- Prefer pd.read_excel(..., engine='openpyxl') for .xlsx reading in newer pandas.\")\n",
    "print(\"- Avoid repeatedly opening/writing the same Excel file in loops; use a single ExcelWriter session when writing many sheets.\")\n",
    "print(\"- When writing floats to CSV, consider float_format='%.2f' to control precision and file size.\\n\")\n",
    "\n",
    "print(\"=== DONE: CSV & Excel read/write examples ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ff08c",
   "metadata": {},
   "source": [
    "## Visualization using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca43d4",
   "metadata": {},
   "source": [
    "1. https://www.geeksforgeeks.org/data-visualization/pandas-built-in-data-visualization-ml/\n",
    "\n",
    "2. https://www.geeksforgeeks.org/data-visualization/data-analysis-visualization-python/\n",
    "\n",
    "3. https://www.geeksforgeeks.org/data-science/data-analysis-visualization-python-set-2/\n",
    "\n",
    "4. https://www.geeksforgeeks.org/data-visualization/box-plot-visualization-with-pandas-and-seaborn/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_seaborn_visualization_examples.py\n",
    "# Demonstrates Pandas built-in plotting (matplotlib-backed) and Seaborn visualizations.\n",
    "# Includes: line, bar, hist, box, kde, scatter, heatmap, pairplot, countplot, violin.\n",
    "# Each plot call has comments explaining the function & main parameters.\n",
    "#\n",
    "# Run: python pandas_seaborn_visualization_examples.py\n",
    "# Requires: pandas, matplotlib, seaborn\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create output directory for saved plots (useful in notebook-less environments)\n",
    "OUT_DIR = \"out_plots\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Use seaborn style for nicer default look\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Sample dataset: small sales dataset & random numeric dataset\n",
    "# ===========================\n",
    "# We'll build a small DataFrame that mimics typical analysis data:\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start=\"2023-01-01\", periods=12, freq='M')  # monthly dates\n",
    "sales = np.random.randint(80, 200, size=12)                     # random monthly sales\n",
    "profit = (sales * (np.random.rand(12) * 0.4 + 0.1)).round(2)    # profit = sales * random margin\n",
    "categories = np.random.choice(['A', 'B', 'C'], size=12)         # categorical column\n",
    "customers = np.random.randint(30, 120, size=12)                 # number of customers\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'sales': sales,\n",
    "    'profit': profit,\n",
    "    'category': categories,\n",
    "    'customers': customers\n",
    "})\n",
    "\n",
    "# Also create a slightly larger DataFrame for distribution / pair plots\n",
    "df_big = pd.DataFrame({\n",
    "    'A': np.random.normal(50, 10, size=200),\n",
    "    'B': np.random.normal(30, 5, size=200),\n",
    "    'C': np.random.gamma(2.0, 5.0, size=200),\n",
    "    'category': np.random.choice(['X','Y','Z'], size=200)\n",
    "})\n",
    "\n",
    "print(\"Data samples:\")\n",
    "print(df.head())\n",
    "print(df_big.head(), \"\\n\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 1) LINE PLOT (time series)\n",
    "# ===========================\n",
    "# pandas.DataFrame.plot(kind='line', x=None, y=None, figsize=None, rot=None)\n",
    "# - kind='line' (default) plots continuous lines.\n",
    "# - x: column to use for x-axis, y: column(s) for y-axis. If not provided, index used for x.\n",
    "# - figsize: (width, height) in inches; rot: rotate x tick labels.\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = df.plot(kind='line', x='date', y='sales', marker='o', title='Monthly Sales', legend=False)\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Sales')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"line_monthly_sales.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 2) BAR PLOT (categorical comparisons)\n",
    "# ===========================\n",
    "# df.plot(kind='bar', x=..., y=..., stacked=False)\n",
    "# - Useful for comparing values across categories or time buckets.\n",
    "monthly_by_cat = df.groupby(df['date'].dt.strftime('%b'))['sales'].sum()  # sales per month name\n",
    "plt.figure(figsize=(8,4))\n",
    "monthly_by_cat.plot(kind='bar', rot=45, title='Sales by Month (bar)')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"bar_sales_by_month.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 3) HISTOGRAM (distribution)\n",
    "# ===========================\n",
    "# df['col'].plot(kind='hist', bins=..., density=False)\n",
    "# - bins: number of histogram bars. density=True -> normalized (PDF).\n",
    "plt.figure(figsize=(6,4))\n",
    "df_big['A'].plot(kind='hist', bins=20, title='Distribution of A (hist)')\n",
    "plt.xlabel('A')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"hist_A.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Seaborn also offers kde/hist combined with sns.histplot\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_big['A'], bins=20, kde=True)\n",
    "plt.title('A distribution (hist + KDE)')\n",
    "plt.savefig(os.path.join(OUT_DIR, \"hist_kde_A.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 4) KERNEL DENSITY ESTIMATE (KDE)\n",
    "# ===========================\n",
    "# df['col'].plot(kind='kde') or sns.kdeplot\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.kdeplot(df_big['B'], fill=True)\n",
    "plt.title('KDE of B')\n",
    "plt.savefig(os.path.join(OUT_DIR, \"kde_B.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 5) BOX PLOT (summary of distribution: median, quartiles, outliers)\n",
    "# ===========================\n",
    "# pandas: df.boxplot(column=[...], by=..., grid=True)\n",
    "# seaborn: sns.boxplot(x=..., y=..., data=...)\n",
    "# Key parameters: notch (show notch), whis (range for whiskers), showfliers (outliers)\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(x='category', y='sales', data=df)   # compare sales across categories\n",
    "plt.title('Boxplot of Sales by Category')\n",
    "plt.savefig(os.path.join(OUT_DIR, \"box_sales_by_category.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 6) VIOLIN PLOT (distribution + density per category)\n",
    "# ===========================\n",
    "# seaborn violinplot provides density shape + quartiles\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.violinplot(x='category', y='profit', data=df, inner='quartile')\n",
    "plt.title('Violin plot of Profit by Category')\n",
    "plt.savefig(os.path.join(OUT_DIR, \"violin_profit_by_category.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 7) SCATTER PLOT & REGRESSION LINE\n",
    "# ===========================\n",
    "# df.plot(kind='scatter', x=..., y=..., s=marker_size)\n",
    "# seaborn: sns.scatterplot and sns.regplot (adds a regression line)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x='sales', y='profit', data=df, hue='category', s=80)\n",
    "plt.title('Sales vs Profit (scatter)')\n",
    "plt.savefig(os.path.join(OUT_DIR, \"scatter_sales_profit.png\"))\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.regplot(x='sales', y='profit', data=df, ci=None)  # ci=None hides confidence interval\n",
    "plt.title('Sales vs Profit (regression)')\n",
    "plt.savefig(os.path.join(OUT_DIR, \"regplot_sales_profit.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 8) PAIRPLOT (pairwise relationships)\n",
    "# ===========================\n",
    "# sns.pairplot(data, vars=..., hue=..., diag_kind='hist'|'kde')\n",
    "# - Useful for small numeric datasets to inspect pairwise correlations & distributions\n",
    "plt.figure()\n",
    "pp = sns.pairplot(df_big, vars=['A','B','C'], hue='category', diag_kind='kde', corner=True)\n",
    "pp.fig.suptitle('Pairplot of numeric columns (df_big)', y=1.02)\n",
    "pp.fig.savefig(os.path.join(OUT_DIR, \"pairplot_df_big.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 9) HEATMAP of correlation matrix\n",
    "# ===========================\n",
    "# Compute correlation matrix: df.corr()\n",
    "# sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "corr = df_big[['A','B','C']].corr()\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation matrix (A, B, C)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"heatmap_corr.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 10) COUNTPLOT (counts of categorical variable)\n",
    "# ===========================\n",
    "# sns.countplot(x='category', data=df_big) — simpler than value_counts+bar plot\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='category', data=df_big, order=sorted(df_big['category'].unique()))\n",
    "plt.title('Category counts (df_big)')\n",
    "plt.savefig(os.path.join(OUT_DIR, \"countplot_category.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 11) BAR PLOT FROM AGGREGATION (groupby + plot)\n",
    "# ===========================\n",
    "# df.groupby('cat')['val'].agg('sum').plot(kind='bar')\n",
    "agg = df.groupby(df['date'].dt.year)['sales'].sum()  # group by year (here all same year but demo)\n",
    "plt.figure(figsize=(6,4))\n",
    "agg.plot(kind='bar', title='Total Sales by Year (grouped)')\n",
    "plt.ylabel('Sales')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"bar_groupby_year.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 12) BOX PLOT USING PANDAS .plot.box (alternative)\n",
    "# ===========================\n",
    "plt.figure(figsize=(6,4))\n",
    "df_big[['A','B','C']].plot.box(title='Box plot (pandas.plot.box)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pandas_box_df_big.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 13) Styling & saving tips (example)\n",
    "# ===========================\n",
    "# - Use figsize to control size; dpi parameter when saving to increase resolution.\n",
    "# - Use plt.tight_layout() to avoid overlapping labels.\n",
    "# - For presentation, choose fonts, colors via seaborn.set_context / matplotlib rcParams.\n",
    "# Example: save high-res figure\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_big['C'], bins=25, kde=True)\n",
    "plt.title('C distribution high-res')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"hist_C_highres.png\"), dpi=200)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# DONE — summary printout\n",
    "# ===========================\n",
    "print(\"Plots saved to folder:\", OUT_DIR)\n",
    "print(\"Examples created: line, bar, hist, kde, box, violin, scatter, regplot, pairplot, heatmap, countplot, box (pandas).\")\n",
    "print(\"\\nTips:\")\n",
    "print(\"- Prefer seaborn for quick statistical visualizations (boxplot, violin, pairplot, heatmap).\")\n",
    "print(\"- Use pandas .plot for quick exploratory plots when working directly from DataFrame.\")\n",
    "print(\"- When working interactively (Jupyter), use %matplotlib inline or notebook and call plt.show() to render.\")\n",
    "print(\"- For large datasets consider sampling before pairplot or plotting aggregated summaries instead of raw scatter.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b028b41",
   "metadata": {},
   "source": [
    "## Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca595489",
   "metadata": {},
   "source": [
    "1. https://www.geeksforgeeks.org/python/how-to-do-a-vlookup-in-python-using-pandas/\n",
    "\n",
    "2. https://www.geeksforgeeks.org/python/convert-csv-to-html-table-in-python/\n",
    "\n",
    "3. https://www.geeksforgeeks.org/data-visualization/kde-plot-visualization-with-pandas-and-seaborn/\n",
    "\n",
    "4. https://www.geeksforgeeks.org/python/analyzing-selling-price-of-used-cars-using-python/\n",
    "\n",
    "5. https://www.geeksforgeeks.org/pandas/add-css-to-the-jupyter-notebook-using-pandas/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
